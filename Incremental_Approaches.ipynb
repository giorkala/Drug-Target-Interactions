{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import os\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # this is for making a model like every other in scikit\n",
    "# from sklearn.decomposition import TruncatedSVD as tSVD\n",
    "\n",
    "import  matplotlib.pyplot as plt\n",
    "random_seed = 2019\n",
    "np.random.seed(random_seed)\n",
    "nfolds=4\n",
    "njobs =5\n",
    "# RESCALE_FACTOR = 10.0 # 0-10000 -> 0-1000 # no need for that, we use -logIC50 instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data file from disk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 110 targets and 23167 compounds currently loaded with 56392 interactions.\n",
      "A DTI matrix would be 2.213% dense!\n",
      "23167 fingerprints were loaded!\n",
      "Stats for values : -4.604582905766776 | 2.5887050795505413\n"
     ]
    }
   ],
   "source": [
    "Interactions_train = []    \n",
    "with open(\"Interactions_Trainset.tab\",'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        # 'Target-ID', 'Compound-ID', 'pIC50'  \n",
    "        Interactions_train.append( [tokens[0], tokens[1], float(tokens[2]) ])\n",
    "\n",
    "Interactions_valid = []        \n",
    "with open(\"Interactions_Validset.tab\",'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        # 'Target-ID', 'Compound-ID', 'pIC50'  \n",
    "        Interactions_valid.append( [tokens[0], tokens[1], float(tokens[2]) ])\n",
    "\n",
    "Interactions = [x for x in Interactions_train]\n",
    "Interactions.extend(Interactions_valid)\n",
    "# we use a dataframe to quickly sort targets wrt #compounds:\n",
    "DF = pd.DataFrame( Interactions, columns =['Target-ID', 'Compound-ID','Std-value']) \n",
    "temp = DF.groupby(['Target-ID']).agg('count').sort_values(by='Compound-ID') # count the number of molecules\n",
    "Targets = list(temp.index)\n",
    "Compounds = np.unique(DF['Compound-ID'])\n",
    "del temp, DF\n",
    "\n",
    "nT=len(Targets); nC=len(Compounds)\n",
    "\n",
    "print(\"There are {0} targets and {1} compounds currently loaded with {2} interactions.\".format(nT,nC,len(Interactions)))\n",
    "print(\"A DTI matrix would be {0:.4}% dense!\".format(100.0*len(Interactions)/nT/nC ))\n",
    "\n",
    "# first we need to prepare each fp as a feature vector\n",
    "Fingerprints={} # this contains one list per fingerprint - not efficient...\n",
    "with open('Compound_Fingerprints.tab', 'r') as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        # each line is Comp-ID, SMILES, FP\n",
    "        tokens = line.split()\n",
    "        # we keep only those compounds which have FPs\n",
    "        if tokens[2] != 'NOFP':\n",
    "            fp = [int(c) for c in tokens[2] ]\n",
    "            Fingerprints[ tokens[0] ] = fp\n",
    "print(\"%d fingerprints were loaded!\" % len(Fingerprints))\n",
    "\n",
    "# data standardisation - no need after using pIC50 !\n",
    "values = [x[2] for x in Interactions]\n",
    "print(\"Stats for values : {0} | {1}\".format(np.mean(values), np.std(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MTL with Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23167, 110)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import sem\n",
    "from scipy.stats import t as tstat\n",
    "from keras import Model\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from time import time\n",
    "\n",
    "def mulpredict(model, x_test, Ntargets, N=10, conf_flag=False):\n",
    "    preds = np.zeros( (N, Ntargets) )\n",
    "    for i in range(N):\n",
    "        preds[i,:] = [ x[0][0] for x in model.predict( x_test ) ]\n",
    "    # we need the column-wise average of this matrix\n",
    "    if conf_flag:\n",
    "        std_err = sem(preds, axis=0)\n",
    "        h = std_err * tstat.ppf((1 + 0.95) / 2, len(preds) - 1)\n",
    "        return np.mean(preds, axis=0), h\n",
    "    else:\n",
    "        return np.mean(preds, axis=0) \n",
    "    \n",
    "def Evaluate(Inter_list, Comp_list, Model, Fingerprints, Ntar=110, Niter=10):\n",
    "    \n",
    "    Predictions = []\n",
    "    Percomp = {} # contains dicts with lists: (target: [true, pred_NN] )\n",
    "    for test_case in Comp_list:\n",
    "        Percomp[ test_case ] = {}\n",
    "        for tokens in Inter_list:\n",
    "            if tokens[1]==test_case:\n",
    "                # CID-TID -> [true_val]\n",
    "                Percomp[test_case][ tokens[0] ] = [ tokens[2] ]\n",
    "        if len(Percomp[ test_case ])>0:\n",
    "            # we've got some values for this compound, now produce predictions:\n",
    "            preds = mulpredict(Model, np.array( Fingerprints[test_case]).reshape(1,-1), Ntar, Niter)\n",
    "            \n",
    "            for target in Percomp[test_case]: \n",
    "                Percomp[test_case][target].append( preds[Labels_Targ[target]])\n",
    "                Predictions.append( [target, test_case, Percomp[test_case][target][0], Percomp[test_case][target][1] ])\n",
    "\n",
    "        if len(Predictions) % 1000 == 0:\n",
    "            r2 = r2_score([x[2] for x in Predictions], [x[3] for x in Predictions])\n",
    "            print(f\"\\rMore than \", len(Predictions),\" predictions have been parsed. Mean performance so far =\",r2, end=\" \")\n",
    "#            print(f\"\\rMore than\", list(Compounds).index(test_case) ,\"compounds have been parsed. Mean score =\", str(r2_score(Y_true, Y_MTLD))[:7] , end =\" \")\n",
    "    print(\" \")\n",
    "    r2 = r2_score([x[2] for x in Predictions], [x[3] for x in Predictions])\n",
    "    print(\"Performance for MTL-D NN = %f\" % r2)\n",
    "    return Predictions\n",
    "\n",
    "def masked_loss_function(y_true, y_pred):\n",
    "    mask = K.cast(K.not_equal(y_true, 10), K.floatx())\n",
    "    return keras.losses.mean_squared_error(y_true * mask, y_pred * mask)\n",
    "\n",
    "def MTL_Drop( wsl, whl, drop_rate=0.1, lr=0.001):\n",
    "    inputs = keras.Input(shape=(2048,))\n",
    "    sharedlayer = keras.layers.Dense(wsl, activation='tanh' )(inputs) \n",
    "    dropout= keras.layers.Dropout(drop_rate)(sharedlayer, training=True)\n",
    "    myinit = keras.initializers.Constant(-4.)\n",
    "    hidden = []\n",
    "    for i in range(len(Targets)):\n",
    "        hl = Dense(units=whl,  activation='tanh', kernel_regularizer=regularizers.l2(0.02) )(dropout)\n",
    "        hidden.append( Dense(1, kernel_initializer=myinit, activity_regularizer=regularizers.l1(0.001) )(hl) )\n",
    "\n",
    "    MTL=Model(inputs=inputs, outputs=hidden)\n",
    "    MTL.compile(loss=masked_loss_function, optimizer=keras.optimizers.adam(lr=lr))\n",
    "    return MTL\n",
    "\n",
    "Labels_Targ = dict()\n",
    "indx=0\n",
    "for x in Targets:\n",
    "    Labels_Targ[x]=indx\n",
    "    indx+=1\n",
    "    \n",
    "Labels_Comp = dict()\n",
    "indx=0\n",
    "for x in Compounds:\n",
    "    Labels_Comp[x]=indx\n",
    "    indx+=1\n",
    "\n",
    "# Initialize sparse matrix - this will be binary\n",
    "DTI = 10*np.ones((nC,nT),dtype=float)\n",
    "\n",
    "for edge in Interactions_train:\n",
    "    # each edge has \"target-compound-value-active\"\n",
    "    DTI[ Labels_Comp[edge[1]], Labels_Targ[edge[0]] ] = edge[2]\n",
    "DTI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 69s - loss: 225.0014 - dense_1108_loss: 0.3735 - dense_1110_loss: 0.2950 - dense_1112_loss: 0.4373 - dense_1114_loss: 0.5223 - dense_1116_loss: 0.3310 - dense_1118_loss: 0.2654 - dense_1120_loss: 0.4485 - dense_1122_loss: 0.4884 - dense_1124_loss: 0.4945 - dense_1126_loss: 0.4334 - dense_1128_loss: 0.7410 - dense_1130_loss: 0.4749 - dense_1132_loss: 0.6846 - dense_1134_loss: 0.4410 - dense_1136_loss: 0.4587 - dense_1138_loss: 0.4664 - dense_1140_loss: 0.6122 - dense_1142_loss: 0.4449 - dense_1144_loss: 0.4940 - dense_1146_loss: 0.5934 - dense_1148_loss: 0.4092 - dense_1150_loss: 0.5820 - dense_1152_loss: 0.4186 - dense_1154_loss: 0.5210 - dense_1156_loss: 0.3573 - dense_1158_loss: 0.2222 - dense_1160_loss: 0.5185 - dense_1162_loss: 0.3766 - dense_1164_loss: 0.3362 - dense_1166_loss: 0.4548 - dense_1168_loss: 0.4720 - dense_1170_loss: 0.6702 - dense_1172_loss: 0.6866 - dense_1174_loss: 0.4868 - dense_1176_loss: 0.5597 - dense_1178_loss: 0.9479 - dense_1180_loss: 0.8491 - dense_1182_loss: 0.6781 - dense_1184_loss: 0.5296 - dense_1186_loss: 0.6607 - dense_1188_loss: 0.7414 - dense_1190_loss: 0.4906 - dense_1192_loss: 0.9521 - dense_1194_loss: 0.3585 - dense_1196_loss: 0.5254 - dense_1198_loss: 0.9289 - dense_1200_loss: 0.5668 - dense_1202_loss: 0.4225 - dense_1204_loss: 0.5660 - dense_1206_loss: 0.3414 - dense_1208_loss: 0.8042 - dense_1210_loss: 0.5419 - dense_1212_loss: 0.7248 - dense_1214_loss: 0.6973 - dense_1216_loss: 0.7271 - dense_1218_loss: 0.7020 - dense_1220_loss: 0.6384 - dense_1222_loss: 0.7189 - dense_1224_loss: 0.3750 - dense_1226_loss: 0.6912 - dense_1228_loss: 0.8032 - dense_1230_loss: 0.6555 - dense_1232_loss: 0.8078 - dense_1234_loss: 0.5621 - dense_1236_loss: 0.7666 - dense_1238_loss: 0.8198 - dense_1240_loss: 0.9054 - dense_1242_loss: 1.0517 - dense_1244_loss: 0.9091 - dense_1246_loss: 0.9901 - dense_1248_loss: 1.0109 - dense_1250_loss: 0.7412 - dense_1252_loss: 1.0524 - dense_1254_loss: 1.0712 - dense_1256_loss: 0.9820 - dense_1258_loss: 1.1264 - dense_1260_loss: 0.6646 - dense_1262_loss: 1.1536 - dense_1264_loss: 1.1694 - dense_1266_loss: 1.0424 - dense_1268_loss: 0.8089 - dense_1270_loss: 0.9979 - dense_1272_loss: 1.1339 - dense_1274_loss: 1.2214 - dense_1276_loss: 0.7070 - dense_1278_loss: 1.3388 - dense_1280_loss: 0.6691 - dense_1282_loss: 1.2519 - dense_1284_loss: 0.8794 - dense_1286_loss: 0.9377 - dense_1288_loss: 1.1441 - dense_1290_loss: 1.5053 - dense_1292_loss: 1.6161 - dense_1294_loss: 0.6658 - dense_1296_loss: 0.7943 - dense_1298_loss: 1.0777 - dense_1300_loss: 1.0949 - dense_1302_loss: 1.5791 - dense_1304_loss: 1.3115 - dense_1306_loss: 1.4760 - dense_1308_loss: 1.2414 - dense_1310_loss: 1.4840 - dense_1312_loss: 1.5389 - dense_1314_loss: 1.4392 - dense_1316_loss: 1.7667 - dense_1318_loss: 1.6064 - dense_1320_loss: 1.6870 - dense_1322_loss: 1.9518 - dense_1324_loss: 2.7972 - dense_1326_loss: 3.0917\n",
      "Epoch 2/30\n",
      " - 18s - loss: 113.8301 - dense_1108_loss: 0.0550 - dense_1110_loss: 0.0734 - dense_1112_loss: 0.0629 - dense_1114_loss: 0.0637 - dense_1116_loss: 0.1034 - dense_1118_loss: 0.0347 - dense_1120_loss: 0.0910 - dense_1122_loss: 0.0353 - dense_1124_loss: 0.1377 - dense_1126_loss: 0.1344 - dense_1128_loss: 0.0323 - dense_1130_loss: 0.2505 - dense_1132_loss: 0.0744 - dense_1134_loss: 0.2292 - dense_1136_loss: 0.1130 - dense_1138_loss: 0.2996 - dense_1140_loss: 0.0716 - dense_1142_loss: 0.0736 - dense_1144_loss: 0.1130 - dense_1146_loss: 0.1419 - dense_1148_loss: 0.1487 - dense_1150_loss: 0.0615 - dense_1152_loss: 0.1028 - dense_1154_loss: 0.0336 - dense_1156_loss: 0.0831 - dense_1158_loss: 0.0383 - dense_1160_loss: 0.0635 - dense_1162_loss: 0.1976 - dense_1164_loss: 0.1351 - dense_1166_loss: 0.1817 - dense_1168_loss: 0.1272 - dense_1170_loss: 0.1386 - dense_1172_loss: 0.0806 - dense_1174_loss: 0.0150 - dense_1176_loss: 0.1289 - dense_1178_loss: 0.1504 - dense_1180_loss: 0.0672 - dense_1182_loss: 0.1817 - dense_1184_loss: 0.1882 - dense_1186_loss: 0.0724 - dense_1188_loss: 0.3955 - dense_1190_loss: 0.0872 - dense_1192_loss: 0.1254 - dense_1194_loss: 0.0915 - dense_1196_loss: 0.2596 - dense_1198_loss: 0.1104 - dense_1200_loss: 0.0776 - dense_1202_loss: 0.1436 - dense_1204_loss: 0.1607 - dense_1206_loss: 0.0345 - dense_1208_loss: 0.2406 - dense_1210_loss: 0.2589 - dense_1212_loss: 0.2685 - dense_1214_loss: 0.1932 - dense_1216_loss: 0.1380 - dense_1218_loss: 0.1969 - dense_1220_loss: 0.1325 - dense_1222_loss: 0.2494 - dense_1224_loss: 0.0717 - dense_1226_loss: 0.1770 - dense_1228_loss: 0.0792 - dense_1230_loss: 0.0874 - dense_1232_loss: 0.1763 - dense_1234_loss: 0.1219 - dense_1236_loss: 0.0888 - dense_1238_loss: 0.2020 - dense_1240_loss: 0.1426 - dense_1242_loss: 0.2682 - dense_1244_loss: 0.1052 - dense_1246_loss: 0.1183 - dense_1248_loss: 0.1289 - dense_1250_loss: 0.1813 - dense_1252_loss: 0.1901 - dense_1254_loss: 0.1967 - dense_1256_loss: 0.2237 - dense_1258_loss: 0.2402 - dense_1260_loss: 0.1998 - dense_1262_loss: 0.1528 - dense_1264_loss: 0.1664 - dense_1266_loss: 0.2468 - dense_1268_loss: 0.2094 - dense_1270_loss: 0.1885 - dense_1272_loss: 0.2920 - dense_1274_loss: 0.2964 - dense_1276_loss: 0.1606 - dense_1278_loss: 0.2942 - dense_1280_loss: 0.1296 - dense_1282_loss: 0.1954 - dense_1284_loss: 0.1489 - dense_1286_loss: 0.1255 - dense_1288_loss: 0.1894 - dense_1290_loss: 0.3062 - dense_1292_loss: 0.0099 - dense_1294_loss: 0.0929 - dense_1296_loss: 0.1285 - dense_1298_loss: 0.2068 - dense_1300_loss: 0.2597 - dense_1302_loss: 0.3411 - dense_1304_loss: 0.3512 - dense_1306_loss: 0.4512 - dense_1308_loss: 0.2849 - dense_1310_loss: 0.3521 - dense_1312_loss: 0.5375 - dense_1314_loss: 0.2335 - dense_1316_loss: 0.2754 - dense_1318_loss: 0.3580 - dense_1320_loss: 0.4975 - dense_1322_loss: 0.5562 - dense_1324_loss: 0.4276 - dense_1326_loss: 0.7113\n",
      "Epoch 3/30\n",
      " - 18s - loss: 80.7742 - dense_1108_loss: 0.0190 - dense_1110_loss: 0.0169 - dense_1112_loss: 0.0149 - dense_1114_loss: 0.0115 - dense_1116_loss: 0.0266 - dense_1118_loss: 0.0193 - dense_1120_loss: 0.0372 - dense_1122_loss: 0.0169 - dense_1124_loss: 0.0408 - dense_1126_loss: 0.0269 - dense_1128_loss: 0.0176 - dense_1130_loss: 0.0420 - dense_1132_loss: 0.0251 - dense_1134_loss: 0.0512 - dense_1136_loss: 0.0329 - dense_1138_loss: 0.0522 - dense_1140_loss: 0.0280 - dense_1142_loss: 0.0241 - dense_1144_loss: 0.0395 - dense_1146_loss: 0.0391 - dense_1148_loss: 0.0296 - dense_1150_loss: 0.0253 - dense_1152_loss: 0.0437 - dense_1154_loss: 0.0175 - dense_1156_loss: 0.0324 - dense_1158_loss: 0.0243 - dense_1160_loss: 0.0228 - dense_1162_loss: 0.0260 - dense_1164_loss: 0.0274 - dense_1166_loss: 0.0390 - dense_1168_loss: 0.0301 - dense_1170_loss: 0.0309 - dense_1172_loss: 0.0487 - dense_1174_loss: 0.0175 - dense_1176_loss: 0.0339 - dense_1178_loss: 0.0614 - dense_1180_loss: 0.0358 - dense_1182_loss: 0.0543 - dense_1184_loss: 0.0266 - dense_1186_loss: 0.0420 - dense_1188_loss: 0.0620 - dense_1190_loss: 0.0472 - dense_1192_loss: 0.0444 - dense_1194_loss: 0.0401 - dense_1196_loss: 0.0599 - dense_1198_loss: 0.0386 - dense_1200_loss: 0.0372 - dense_1202_loss: 0.0498 - dense_1204_loss: 0.0643 - dense_1206_loss: 0.0299 - dense_1208_loss: 0.0685 - dense_1210_loss: 0.0789 - dense_1212_loss: 0.0588 - dense_1214_loss: 0.0504 - dense_1216_loss: 0.0501 - dense_1218_loss: 0.0539 - dense_1220_loss: 0.0652 - dense_1222_loss: 0.0828 - dense_1224_loss: 0.0367 - dense_1226_loss: 0.0762 - dense_1228_loss: 0.0473 - dense_1230_loss: 0.0742 - dense_1232_loss: 0.1489 - dense_1234_loss: 0.0879 - dense_1236_loss: 0.0515 - dense_1238_loss: 0.0641 - dense_1240_loss: 0.0675 - dense_1242_loss: 0.0769 - dense_1244_loss: 0.0773 - dense_1246_loss: 0.0731 - dense_1248_loss: 0.0591 - dense_1250_loss: 0.0753 - dense_1252_loss: 0.1011 - dense_1254_loss: 0.0953 - dense_1256_loss: 0.1197 - dense_1258_loss: 0.1144 - dense_1260_loss: 0.1099 - dense_1262_loss: 0.0854 - dense_1264_loss: 0.0932 - dense_1266_loss: 0.0810 - dense_1268_loss: 0.0901 - dense_1270_loss: 0.0863 - dense_1272_loss: 0.1481 - dense_1274_loss: 0.1568 - dense_1276_loss: 0.0627 - dense_1278_loss: 0.1525 - dense_1280_loss: 0.0778 - dense_1282_loss: 0.1285 - dense_1284_loss: 0.1143 - dense_1286_loss: 0.1012 - dense_1288_loss: 0.1187 - dense_1290_loss: 0.1720 - dense_1292_loss: 0.0092 - dense_1294_loss: 0.0814 - dense_1296_loss: 0.0983 - dense_1298_loss: 0.0932 - dense_1300_loss: 0.1526 - dense_1302_loss: 0.2221 - dense_1304_loss: 0.2027 - dense_1306_loss: 0.2357 - dense_1308_loss: 0.1651 - dense_1310_loss: 0.2112 - dense_1312_loss: 0.2838 - dense_1314_loss: 0.1980 - dense_1316_loss: 0.1983 - dense_1318_loss: 0.2989 - dense_1320_loss: 0.2951 - dense_1322_loss: 0.4016 - dense_1324_loss: 0.3528 - dense_1326_loss: 0.6181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30\n",
      " - 18s - loss: 62.7786 - dense_1108_loss: 0.0152 - dense_1110_loss: 0.0146 - dense_1112_loss: 0.0183 - dense_1114_loss: 0.0090 - dense_1116_loss: 0.0175 - dense_1118_loss: 0.0139 - dense_1120_loss: 0.0297 - dense_1122_loss: 0.0162 - dense_1124_loss: 0.0364 - dense_1126_loss: 0.0213 - dense_1128_loss: 0.0138 - dense_1130_loss: 0.0300 - dense_1132_loss: 0.0213 - dense_1134_loss: 0.0421 - dense_1136_loss: 0.0301 - dense_1138_loss: 0.0395 - dense_1140_loss: 0.0199 - dense_1142_loss: 0.0166 - dense_1144_loss: 0.0291 - dense_1146_loss: 0.0309 - dense_1148_loss: 0.0324 - dense_1150_loss: 0.0287 - dense_1152_loss: 0.0351 - dense_1154_loss: 0.0191 - dense_1156_loss: 0.0191 - dense_1158_loss: 0.0219 - dense_1160_loss: 0.0210 - dense_1162_loss: 0.0225 - dense_1164_loss: 0.0231 - dense_1166_loss: 0.0268 - dense_1168_loss: 0.0180 - dense_1170_loss: 0.0184 - dense_1172_loss: 0.0367 - dense_1174_loss: 0.0141 - dense_1176_loss: 0.0362 - dense_1178_loss: 0.0496 - dense_1180_loss: 0.0291 - dense_1182_loss: 0.0324 - dense_1184_loss: 0.0256 - dense_1186_loss: 0.0279 - dense_1188_loss: 0.0532 - dense_1190_loss: 0.0326 - dense_1192_loss: 0.0399 - dense_1194_loss: 0.0335 - dense_1196_loss: 0.0390 - dense_1198_loss: 0.0341 - dense_1200_loss: 0.0252 - dense_1202_loss: 0.0297 - dense_1204_loss: 0.0425 - dense_1206_loss: 0.0228 - dense_1208_loss: 0.0630 - dense_1210_loss: 0.0438 - dense_1212_loss: 0.0540 - dense_1214_loss: 0.0430 - dense_1216_loss: 0.0425 - dense_1218_loss: 0.0473 - dense_1220_loss: 0.0480 - dense_1222_loss: 0.0705 - dense_1224_loss: 0.0290 - dense_1226_loss: 0.0629 - dense_1228_loss: 0.0359 - dense_1230_loss: 0.0654 - dense_1232_loss: 0.0866 - dense_1234_loss: 0.0501 - dense_1236_loss: 0.0334 - dense_1238_loss: 0.0598 - dense_1240_loss: 0.0633 - dense_1242_loss: 0.0639 - dense_1244_loss: 0.0686 - dense_1246_loss: 0.0497 - dense_1248_loss: 0.0460 - dense_1250_loss: 0.0665 - dense_1252_loss: 0.0695 - dense_1254_loss: 0.0670 - dense_1256_loss: 0.0797 - dense_1258_loss: 0.0985 - dense_1260_loss: 0.0535 - dense_1262_loss: 0.0733 - dense_1264_loss: 0.0793 - dense_1266_loss: 0.0801 - dense_1268_loss: 0.0699 - dense_1270_loss: 0.0800 - dense_1272_loss: 0.0935 - dense_1274_loss: 0.1083 - dense_1276_loss: 0.0468 - dense_1278_loss: 0.1344 - dense_1280_loss: 0.0759 - dense_1282_loss: 0.0998 - dense_1284_loss: 0.0942 - dense_1286_loss: 0.0797 - dense_1288_loss: 0.1082 - dense_1290_loss: 0.1312 - dense_1292_loss: 0.0095 - dense_1294_loss: 0.0600 - dense_1296_loss: 0.0828 - dense_1298_loss: 0.0834 - dense_1300_loss: 0.1180 - dense_1302_loss: 0.1671 - dense_1304_loss: 0.1487 - dense_1306_loss: 0.1526 - dense_1308_loss: 0.1261 - dense_1310_loss: 0.1734 - dense_1312_loss: 0.2256 - dense_1314_loss: 0.1838 - dense_1316_loss: 0.1645 - dense_1318_loss: 0.2016 - dense_1320_loss: 0.2200 - dense_1322_loss: 0.2912 - dense_1324_loss: 0.3151 - dense_1326_loss: 0.4476\n",
      "Epoch 5/30\n",
      " - 18s - loss: 49.3088 - dense_1108_loss: 0.0172 - dense_1110_loss: 0.0151 - dense_1112_loss: 0.0205 - dense_1114_loss: 0.0103 - dense_1116_loss: 0.0181 - dense_1118_loss: 0.0139 - dense_1120_loss: 0.0304 - dense_1122_loss: 0.0202 - dense_1124_loss: 0.0295 - dense_1126_loss: 0.0206 - dense_1128_loss: 0.0151 - dense_1130_loss: 0.0336 - dense_1132_loss: 0.0209 - dense_1134_loss: 0.0380 - dense_1136_loss: 0.0325 - dense_1138_loss: 0.0327 - dense_1140_loss: 0.0164 - dense_1142_loss: 0.0199 - dense_1144_loss: 0.0301 - dense_1146_loss: 0.0287 - dense_1148_loss: 0.0400 - dense_1150_loss: 0.0260 - dense_1152_loss: 0.0292 - dense_1154_loss: 0.0166 - dense_1156_loss: 0.0180 - dense_1158_loss: 0.0177 - dense_1160_loss: 0.0194 - dense_1162_loss: 0.0217 - dense_1164_loss: 0.0254 - dense_1166_loss: 0.0279 - dense_1168_loss: 0.0217 - dense_1170_loss: 0.0219 - dense_1172_loss: 0.0254 - dense_1174_loss: 0.0116 - dense_1176_loss: 0.0454 - dense_1178_loss: 0.0504 - dense_1180_loss: 0.0284 - dense_1182_loss: 0.0408 - dense_1184_loss: 0.0219 - dense_1186_loss: 0.0327 - dense_1188_loss: 0.0487 - dense_1190_loss: 0.0445 - dense_1192_loss: 0.0328 - dense_1194_loss: 0.0249 - dense_1196_loss: 0.0334 - dense_1198_loss: 0.0344 - dense_1200_loss: 0.0247 - dense_1202_loss: 0.0257 - dense_1204_loss: 0.0334 - dense_1206_loss: 0.0255 - dense_1208_loss: 0.0572 - dense_1210_loss: 0.0532 - dense_1212_loss: 0.0455 - dense_1214_loss: 0.0386 - dense_1216_loss: 0.0450 - dense_1218_loss: 0.0412 - dense_1220_loss: 0.0445 - dense_1222_loss: 0.0623 - dense_1224_loss: 0.0305 - dense_1226_loss: 0.0477 - dense_1228_loss: 0.0338 - dense_1230_loss: 0.1059 - dense_1232_loss: 0.0577 - dense_1234_loss: 0.0465 - dense_1236_loss: 0.0378 - dense_1238_loss: 0.0558 - dense_1240_loss: 0.0608 - dense_1242_loss: 0.0575 - dense_1244_loss: 0.0661 - dense_1246_loss: 0.0448 - dense_1248_loss: 0.0423 - dense_1250_loss: 0.0580 - dense_1252_loss: 0.0628 - dense_1254_loss: 0.0582 - dense_1256_loss: 0.0638 - dense_1258_loss: 0.0977 - dense_1260_loss: 0.0509 - dense_1262_loss: 0.0723 - dense_1264_loss: 0.0737 - dense_1266_loss: 0.0617 - dense_1268_loss: 0.0644 - dense_1270_loss: 0.0590 - dense_1272_loss: 0.0850 - dense_1274_loss: 0.1020 - dense_1276_loss: 0.0454 - dense_1278_loss: 0.1276 - dense_1280_loss: 0.0691 - dense_1282_loss: 0.0758 - dense_1284_loss: 0.0769 - dense_1286_loss: 0.0708 - dense_1288_loss: 0.0929 - dense_1290_loss: 0.1019 - dense_1292_loss: 0.0099 - dense_1294_loss: 0.0504 - dense_1296_loss: 0.0841 - dense_1298_loss: 0.0664 - dense_1300_loss: 0.0998 - dense_1302_loss: 0.1422 - dense_1304_loss: 0.1267 - dense_1306_loss: 0.1565 - dense_1308_loss: 0.1198 - dense_1310_loss: 0.1554 - dense_1312_loss: 0.2305 - dense_1314_loss: 0.1579 - dense_1316_loss: 0.1443 - dense_1318_loss: 0.1738 - dense_1320_loss: 0.2021 - dense_1322_loss: 0.2441 - dense_1324_loss: 0.2642 - dense_1326_loss: 0.4271\n",
      "Epoch 6/30\n",
      " - 40s - loss: 39.1002 - dense_1108_loss: 0.0150 - dense_1110_loss: 0.0129 - dense_1112_loss: 0.0223 - dense_1114_loss: 0.0088 - dense_1116_loss: 0.0159 - dense_1118_loss: 0.0128 - dense_1120_loss: 0.0300 - dense_1122_loss: 0.0191 - dense_1124_loss: 0.0270 - dense_1126_loss: 0.0188 - dense_1128_loss: 0.0147 - dense_1130_loss: 0.0267 - dense_1132_loss: 0.0201 - dense_1134_loss: 0.0393 - dense_1136_loss: 0.0296 - dense_1138_loss: 0.0315 - dense_1140_loss: 0.0167 - dense_1142_loss: 0.0196 - dense_1144_loss: 0.0274 - dense_1146_loss: 0.0299 - dense_1148_loss: 0.0262 - dense_1150_loss: 0.0275 - dense_1152_loss: 0.0355 - dense_1154_loss: 0.0133 - dense_1156_loss: 0.0202 - dense_1158_loss: 0.0196 - dense_1160_loss: 0.0198 - dense_1162_loss: 0.0188 - dense_1164_loss: 0.0221 - dense_1166_loss: 0.0225 - dense_1168_loss: 0.0197 - dense_1170_loss: 0.0176 - dense_1172_loss: 0.0324 - dense_1174_loss: 0.0161 - dense_1176_loss: 0.0293 - dense_1178_loss: 0.0447 - dense_1180_loss: 0.0330 - dense_1182_loss: 0.0399 - dense_1184_loss: 0.0244 - dense_1186_loss: 0.0334 - dense_1188_loss: 0.0524 - dense_1190_loss: 0.0343 - dense_1192_loss: 0.0359 - dense_1194_loss: 0.0242 - dense_1196_loss: 0.0334 - dense_1198_loss: 0.0331 - dense_1200_loss: 0.0282 - dense_1202_loss: 0.0303 - dense_1204_loss: 0.0317 - dense_1206_loss: 0.0217 - dense_1208_loss: 0.0547 - dense_1210_loss: 0.0530 - dense_1212_loss: 0.0452 - dense_1214_loss: 0.0333 - dense_1216_loss: 0.0370 - dense_1218_loss: 0.0385 - dense_1220_loss: 0.0420 - dense_1222_loss: 0.0597 - dense_1224_loss: 0.0297 - dense_1226_loss: 0.0498 - dense_1228_loss: 0.0307 - dense_1230_loss: 0.0643 - dense_1232_loss: 0.0490 - dense_1234_loss: 0.0408 - dense_1236_loss: 0.0344 - dense_1238_loss: 0.0468 - dense_1240_loss: 0.0591 - dense_1242_loss: 0.0572 - dense_1244_loss: 0.0591 - dense_1246_loss: 0.0466 - dense_1248_loss: 0.0408 - dense_1250_loss: 0.0520 - dense_1252_loss: 0.0604 - dense_1254_loss: 0.0589 - dense_1256_loss: 0.0532 - dense_1258_loss: 0.0794 - dense_1260_loss: 0.0489 - dense_1262_loss: 0.0619 - dense_1264_loss: 0.0679 - dense_1266_loss: 0.0568 - dense_1268_loss: 0.0664 - dense_1270_loss: 0.0566 - dense_1272_loss: 0.0835 - dense_1274_loss: 0.0909 - dense_1276_loss: 0.0393 - dense_1278_loss: 0.1157 - dense_1280_loss: 0.0641 - dense_1282_loss: 0.0739 - dense_1284_loss: 0.0772 - dense_1286_loss: 0.0698 - dense_1288_loss: 0.0855 - dense_1290_loss: 0.0888 - dense_1292_loss: 0.0088 - dense_1294_loss: 0.0509 - dense_1296_loss: 0.0821 - dense_1298_loss: 0.0684 - dense_1300_loss: 0.0937 - dense_1302_loss: 0.1226 - dense_1304_loss: 0.1340 - dense_1306_loss: 0.1414 - dense_1308_loss: 0.1145 - dense_1310_loss: 0.1373 - dense_1312_loss: 0.1932 - dense_1314_loss: 0.1459 - dense_1316_loss: 0.1447 - dense_1318_loss: 0.1895 - dense_1320_loss: 0.1922 - dense_1322_loss: 0.2410 - dense_1324_loss: 0.2468 - dense_1326_loss: 0.3736\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30\n",
      " - 34s - loss: 31.7173 - dense_1108_loss: 0.0158 - dense_1110_loss: 0.0182 - dense_1112_loss: 0.0182 - dense_1114_loss: 0.0106 - dense_1116_loss: 0.0146 - dense_1118_loss: 0.0119 - dense_1120_loss: 0.0292 - dense_1122_loss: 0.0169 - dense_1124_loss: 0.0275 - dense_1126_loss: 0.0160 - dense_1128_loss: 0.0146 - dense_1130_loss: 0.0270 - dense_1132_loss: 0.0189 - dense_1134_loss: 0.0335 - dense_1136_loss: 0.0288 - dense_1138_loss: 0.0301 - dense_1140_loss: 0.0156 - dense_1142_loss: 0.0156 - dense_1144_loss: 0.0265 - dense_1146_loss: 0.0313 - dense_1148_loss: 0.0269 - dense_1150_loss: 0.0294 - dense_1152_loss: 0.0336 - dense_1154_loss: 0.0161 - dense_1156_loss: 0.0163 - dense_1158_loss: 0.0194 - dense_1160_loss: 0.0182 - dense_1162_loss: 0.0189 - dense_1164_loss: 0.0218 - dense_1166_loss: 0.0231 - dense_1168_loss: 0.0179 - dense_1170_loss: 0.0196 - dense_1172_loss: 0.0282 - dense_1174_loss: 0.0168 - dense_1176_loss: 0.0244 - dense_1178_loss: 0.0457 - dense_1180_loss: 0.0288 - dense_1182_loss: 0.0402 - dense_1184_loss: 0.0260 - dense_1186_loss: 0.0275 - dense_1188_loss: 0.0480 - dense_1190_loss: 0.0262 - dense_1192_loss: 0.0324 - dense_1194_loss: 0.0214 - dense_1196_loss: 0.0320 - dense_1198_loss: 0.0362 - dense_1200_loss: 0.0192 - dense_1202_loss: 0.0289 - dense_1204_loss: 0.0302 - dense_1206_loss: 0.0239 - dense_1208_loss: 0.0494 - dense_1210_loss: 0.0454 - dense_1212_loss: 0.0454 - dense_1214_loss: 0.0354 - dense_1216_loss: 0.0375 - dense_1218_loss: 0.0371 - dense_1220_loss: 0.0474 - dense_1222_loss: 0.0552 - dense_1224_loss: 0.0287 - dense_1226_loss: 0.0511 - dense_1228_loss: 0.0322 - dense_1230_loss: 0.0459 - dense_1232_loss: 0.0518 - dense_1234_loss: 0.0393 - dense_1236_loss: 0.0341 - dense_1238_loss: 0.0472 - dense_1240_loss: 0.0589 - dense_1242_loss: 0.0517 - dense_1244_loss: 0.0587 - dense_1246_loss: 0.0422 - dense_1248_loss: 0.0383 - dense_1250_loss: 0.0527 - dense_1252_loss: 0.0657 - dense_1254_loss: 0.0561 - dense_1256_loss: 0.0503 - dense_1258_loss: 0.0855 - dense_1260_loss: 0.0422 - dense_1262_loss: 0.0572 - dense_1264_loss: 0.0563 - dense_1266_loss: 0.0513 - dense_1268_loss: 0.0598 - dense_1270_loss: 0.0545 - dense_1272_loss: 0.0725 - dense_1274_loss: 0.0892 - dense_1276_loss: 0.0418 - dense_1278_loss: 0.1098 - dense_1280_loss: 0.0742 - dense_1282_loss: 0.0793 - dense_1284_loss: 0.0787 - dense_1286_loss: 0.0621 - dense_1288_loss: 0.0867 - dense_1290_loss: 0.0946 - dense_1292_loss: 0.0083 - dense_1294_loss: 0.0526 - dense_1296_loss: 0.0764 - dense_1298_loss: 0.0646 - dense_1300_loss: 0.0911 - dense_1302_loss: 0.1393 - dense_1304_loss: 0.1266 - dense_1306_loss: 0.1318 - dense_1308_loss: 0.1169 - dense_1310_loss: 0.1372 - dense_1312_loss: 0.1823 - dense_1314_loss: 0.1484 - dense_1316_loss: 0.1250 - dense_1318_loss: 0.1538 - dense_1320_loss: 0.1726 - dense_1322_loss: 0.2043 - dense_1324_loss: 0.2374 - dense_1326_loss: 0.3478\n",
      "Epoch 8/30\n",
      " - 41s - loss: 26.4604 - dense_1108_loss: 0.0149 - dense_1110_loss: 0.0147 - dense_1112_loss: 0.0172 - dense_1114_loss: 0.0101 - dense_1116_loss: 0.0134 - dense_1118_loss: 0.0113 - dense_1120_loss: 0.0285 - dense_1122_loss: 0.0138 - dense_1124_loss: 0.0295 - dense_1126_loss: 0.0172 - dense_1128_loss: 0.0136 - dense_1130_loss: 0.0255 - dense_1132_loss: 0.0172 - dense_1134_loss: 0.0354 - dense_1136_loss: 0.0266 - dense_1138_loss: 0.0267 - dense_1140_loss: 0.0135 - dense_1142_loss: 0.0177 - dense_1144_loss: 0.0230 - dense_1146_loss: 0.0309 - dense_1148_loss: 0.0277 - dense_1150_loss: 0.0243 - dense_1152_loss: 0.0296 - dense_1154_loss: 0.0132 - dense_1156_loss: 0.0188 - dense_1158_loss: 0.0170 - dense_1160_loss: 0.0174 - dense_1162_loss: 0.0179 - dense_1164_loss: 0.0243 - dense_1166_loss: 0.0282 - dense_1168_loss: 0.0169 - dense_1170_loss: 0.0206 - dense_1172_loss: 0.0262 - dense_1174_loss: 0.0120 - dense_1176_loss: 0.0302 - dense_1178_loss: 0.0396 - dense_1180_loss: 0.0249 - dense_1182_loss: 0.0344 - dense_1184_loss: 0.0243 - dense_1186_loss: 0.0293 - dense_1188_loss: 0.0475 - dense_1190_loss: 0.0293 - dense_1192_loss: 0.0364 - dense_1194_loss: 0.0186 - dense_1196_loss: 0.0312 - dense_1198_loss: 0.0335 - dense_1200_loss: 0.0201 - dense_1202_loss: 0.0249 - dense_1204_loss: 0.0295 - dense_1206_loss: 0.0211 - dense_1208_loss: 0.0517 - dense_1210_loss: 0.0426 - dense_1212_loss: 0.0427 - dense_1214_loss: 0.0362 - dense_1216_loss: 0.0374 - dense_1218_loss: 0.0351 - dense_1220_loss: 0.0377 - dense_1222_loss: 0.0512 - dense_1224_loss: 0.0239 - dense_1226_loss: 0.0535 - dense_1228_loss: 0.0347 - dense_1230_loss: 0.0519 - dense_1232_loss: 0.0576 - dense_1234_loss: 0.0406 - dense_1236_loss: 0.0359 - dense_1238_loss: 0.0439 - dense_1240_loss: 0.0535 - dense_1242_loss: 0.0554 - dense_1244_loss: 0.0538 - dense_1246_loss: 0.0411 - dense_1248_loss: 0.0362 - dense_1250_loss: 0.0536 - dense_1252_loss: 0.0691 - dense_1254_loss: 0.0578 - dense_1256_loss: 0.0493 - dense_1258_loss: 0.0784 - dense_1260_loss: 0.0411 - dense_1262_loss: 0.0502 - dense_1264_loss: 0.0590 - dense_1266_loss: 0.0505 - dense_1268_loss: 0.0563 - dense_1270_loss: 0.0567 - dense_1272_loss: 0.0788 - dense_1274_loss: 0.0938 - dense_1276_loss: 0.0399 - dense_1278_loss: 0.1128 - dense_1280_loss: 0.0543 - dense_1282_loss: 0.0830 - dense_1284_loss: 0.0741 - dense_1286_loss: 0.0589 - dense_1288_loss: 0.0801 - dense_1290_loss: 0.0877 - dense_1292_loss: 0.0077 - dense_1294_loss: 0.0511 - dense_1296_loss: 0.0678 - dense_1298_loss: 0.0603 - dense_1300_loss: 0.0874 - dense_1302_loss: 0.1139 - dense_1304_loss: 0.1271 - dense_1306_loss: 0.1157 - dense_1308_loss: 0.1080 - dense_1310_loss: 0.1329 - dense_1312_loss: 0.1797 - dense_1314_loss: 0.1263 - dense_1316_loss: 0.1152 - dense_1318_loss: 0.1451 - dense_1320_loss: 0.1790 - dense_1322_loss: 0.2074 - dense_1324_loss: 0.2496 - dense_1326_loss: 0.3469\n",
      "Epoch 9/30\n",
      " - 38s - loss: 22.9193 - dense_1108_loss: 0.0169 - dense_1110_loss: 0.0162 - dense_1112_loss: 0.0183 - dense_1114_loss: 0.0104 - dense_1116_loss: 0.0165 - dense_1118_loss: 0.0105 - dense_1120_loss: 0.0293 - dense_1122_loss: 0.0161 - dense_1124_loss: 0.0379 - dense_1126_loss: 0.0143 - dense_1128_loss: 0.0145 - dense_1130_loss: 0.0221 - dense_1132_loss: 0.0158 - dense_1134_loss: 0.0350 - dense_1136_loss: 0.0268 - dense_1138_loss: 0.0245 - dense_1140_loss: 0.0131 - dense_1142_loss: 0.0223 - dense_1144_loss: 0.0287 - dense_1146_loss: 0.0315 - dense_1148_loss: 0.0307 - dense_1150_loss: 0.0237 - dense_1152_loss: 0.0270 - dense_1154_loss: 0.0147 - dense_1156_loss: 0.0201 - dense_1158_loss: 0.0159 - dense_1160_loss: 0.0158 - dense_1162_loss: 0.0155 - dense_1164_loss: 0.0195 - dense_1166_loss: 0.0239 - dense_1168_loss: 0.0199 - dense_1170_loss: 0.0227 - dense_1172_loss: 0.0295 - dense_1174_loss: 0.0169 - dense_1176_loss: 0.0299 - dense_1178_loss: 0.0405 - dense_1180_loss: 0.0269 - dense_1182_loss: 0.0371 - dense_1184_loss: 0.0222 - dense_1186_loss: 0.0300 - dense_1188_loss: 0.0460 - dense_1190_loss: 0.0338 - dense_1192_loss: 0.0295 - dense_1194_loss: 0.0224 - dense_1196_loss: 0.0371 - dense_1198_loss: 0.0339 - dense_1200_loss: 0.0194 - dense_1202_loss: 0.0233 - dense_1204_loss: 0.0338 - dense_1206_loss: 0.0259 - dense_1208_loss: 0.0483 - dense_1210_loss: 0.0462 - dense_1212_loss: 0.0389 - dense_1214_loss: 0.0315 - dense_1216_loss: 0.0358 - dense_1218_loss: 0.0399 - dense_1220_loss: 0.0372 - dense_1222_loss: 0.0514 - dense_1224_loss: 0.0256 - dense_1226_loss: 0.0520 - dense_1228_loss: 0.0285 - dense_1230_loss: 0.0450 - dense_1232_loss: 0.0664 - dense_1234_loss: 0.0417 - dense_1236_loss: 0.0313 - dense_1238_loss: 0.0416 - dense_1240_loss: 0.0610 - dense_1242_loss: 0.0495 - dense_1244_loss: 0.0639 - dense_1246_loss: 0.0473 - dense_1248_loss: 0.0329 - dense_1250_loss: 0.0554 - dense_1252_loss: 0.0603 - dense_1254_loss: 0.0606 - dense_1256_loss: 0.0504 - dense_1258_loss: 0.0906 - dense_1260_loss: 0.0384 - dense_1262_loss: 0.0503 - dense_1264_loss: 0.0584 - dense_1266_loss: 0.0568 - dense_1268_loss: 0.0696 - dense_1270_loss: 0.0511 - dense_1272_loss: 0.0837 - dense_1274_loss: 0.0963 - dense_1276_loss: 0.0350 - dense_1278_loss: 0.1156 - dense_1280_loss: 0.0571 - dense_1282_loss: 0.0792 - dense_1284_loss: 0.0769 - dense_1286_loss: 0.0590 - dense_1288_loss: 0.0672 - dense_1290_loss: 0.0895 - dense_1292_loss: 0.0072 - dense_1294_loss: 0.0551 - dense_1296_loss: 0.0711 - dense_1298_loss: 0.0630 - dense_1300_loss: 0.0835 - dense_1302_loss: 0.1101 - dense_1304_loss: 0.1155 - dense_1306_loss: 0.1191 - dense_1308_loss: 0.1110 - dense_1310_loss: 0.1292 - dense_1312_loss: 0.1812 - dense_1314_loss: 0.1349 - dense_1316_loss: 0.1627 - dense_1318_loss: 0.1466 - dense_1320_loss: 0.1708 - dense_1322_loss: 0.1921 - dense_1324_loss: 0.2282 - dense_1326_loss: 0.3416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30\n",
      " - 40s - loss: 20.3169 - dense_1108_loss: 0.0145 - dense_1110_loss: 0.0109 - dense_1112_loss: 0.0191 - dense_1114_loss: 0.0084 - dense_1116_loss: 0.0148 - dense_1118_loss: 0.0105 - dense_1120_loss: 0.0271 - dense_1122_loss: 0.0179 - dense_1124_loss: 0.0304 - dense_1126_loss: 0.0189 - dense_1128_loss: 0.0156 - dense_1130_loss: 0.0242 - dense_1132_loss: 0.0168 - dense_1134_loss: 0.0341 - dense_1136_loss: 0.0262 - dense_1138_loss: 0.0271 - dense_1140_loss: 0.0163 - dense_1142_loss: 0.0201 - dense_1144_loss: 0.0263 - dense_1146_loss: 0.0267 - dense_1148_loss: 0.0289 - dense_1150_loss: 0.0223 - dense_1152_loss: 0.0335 - dense_1154_loss: 0.0131 - dense_1156_loss: 0.0174 - dense_1158_loss: 0.0212 - dense_1160_loss: 0.0184 - dense_1162_loss: 0.0151 - dense_1164_loss: 0.0215 - dense_1166_loss: 0.0229 - dense_1168_loss: 0.0166 - dense_1170_loss: 0.0233 - dense_1172_loss: 0.0262 - dense_1174_loss: 0.0160 - dense_1176_loss: 0.0324 - dense_1178_loss: 0.0362 - dense_1180_loss: 0.0292 - dense_1182_loss: 0.0387 - dense_1184_loss: 0.0206 - dense_1186_loss: 0.0283 - dense_1188_loss: 0.0480 - dense_1190_loss: 0.0286 - dense_1192_loss: 0.0285 - dense_1194_loss: 0.0205 - dense_1196_loss: 0.0310 - dense_1198_loss: 0.0334 - dense_1200_loss: 0.0196 - dense_1202_loss: 0.0236 - dense_1204_loss: 0.0359 - dense_1206_loss: 0.0214 - dense_1208_loss: 0.0494 - dense_1210_loss: 0.0414 - dense_1212_loss: 0.0396 - dense_1214_loss: 0.0325 - dense_1216_loss: 0.0380 - dense_1218_loss: 0.0360 - dense_1220_loss: 0.0400 - dense_1222_loss: 0.0529 - dense_1224_loss: 0.0261 - dense_1226_loss: 0.0480 - dense_1228_loss: 0.0276 - dense_1230_loss: 0.0569 - dense_1232_loss: 0.0574 - dense_1234_loss: 0.0468 - dense_1236_loss: 0.0291 - dense_1238_loss: 0.0560 - dense_1240_loss: 0.0495 - dense_1242_loss: 0.0502 - dense_1244_loss: 0.0622 - dense_1246_loss: 0.0467 - dense_1248_loss: 0.0312 - dense_1250_loss: 0.0521 - dense_1252_loss: 0.0634 - dense_1254_loss: 0.0612 - dense_1256_loss: 0.0562 - dense_1258_loss: 0.0821 - dense_1260_loss: 0.0440 - dense_1262_loss: 0.0530 - dense_1264_loss: 0.0557 - dense_1266_loss: 0.0501 - dense_1268_loss: 0.0583 - dense_1270_loss: 0.0601 - dense_1272_loss: 0.0785 - dense_1274_loss: 0.0847 - dense_1276_loss: 0.0364 - dense_1278_loss: 0.1128 - dense_1280_loss: 0.0565 - dense_1282_loss: 0.0748 - dense_1284_loss: 0.0690 - dense_1286_loss: 0.0554 - dense_1288_loss: 0.0697 - dense_1290_loss: 0.0846 - dense_1292_loss: 0.0073 - dense_1294_loss: 0.0553 - dense_1296_loss: 0.0641 - dense_1298_loss: 0.0550 - dense_1300_loss: 0.0881 - dense_1302_loss: 0.1166 - dense_1304_loss: 0.1182 - dense_1306_loss: 0.1166 - dense_1308_loss: 0.0994 - dense_1310_loss: 0.1293 - dense_1312_loss: 0.1793 - dense_1314_loss: 0.1317 - dense_1316_loss: 0.1260 - dense_1318_loss: 0.1391 - dense_1320_loss: 0.1592 - dense_1322_loss: 0.1897 - dense_1324_loss: 0.2534 - dense_1326_loss: 0.3345\n",
      "Epoch 11/30\n",
      " - 41s - loss: 18.2980 - dense_1108_loss: 0.0127 - dense_1110_loss: 0.0102 - dense_1112_loss: 0.0192 - dense_1114_loss: 0.0122 - dense_1116_loss: 0.0142 - dense_1118_loss: 0.0109 - dense_1120_loss: 0.0203 - dense_1122_loss: 0.0148 - dense_1124_loss: 0.0261 - dense_1126_loss: 0.0150 - dense_1128_loss: 0.0151 - dense_1130_loss: 0.0190 - dense_1132_loss: 0.0176 - dense_1134_loss: 0.0279 - dense_1136_loss: 0.0239 - dense_1138_loss: 0.0275 - dense_1140_loss: 0.0133 - dense_1142_loss: 0.0181 - dense_1144_loss: 0.0252 - dense_1146_loss: 0.0537 - dense_1148_loss: 0.0241 - dense_1150_loss: 0.0213 - dense_1152_loss: 0.0273 - dense_1154_loss: 0.0152 - dense_1156_loss: 0.0141 - dense_1158_loss: 0.0184 - dense_1160_loss: 0.0172 - dense_1162_loss: 0.0149 - dense_1164_loss: 0.0169 - dense_1166_loss: 0.0277 - dense_1168_loss: 0.0231 - dense_1170_loss: 0.0206 - dense_1172_loss: 0.0243 - dense_1174_loss: 0.0126 - dense_1176_loss: 0.0272 - dense_1178_loss: 0.0417 - dense_1180_loss: 0.0222 - dense_1182_loss: 0.0351 - dense_1184_loss: 0.0194 - dense_1186_loss: 0.0231 - dense_1188_loss: 0.0457 - dense_1190_loss: 0.0283 - dense_1192_loss: 0.0257 - dense_1194_loss: 0.0181 - dense_1196_loss: 0.0398 - dense_1198_loss: 0.0319 - dense_1200_loss: 0.0170 - dense_1202_loss: 0.0231 - dense_1204_loss: 0.0289 - dense_1206_loss: 0.0228 - dense_1208_loss: 0.0461 - dense_1210_loss: 0.0438 - dense_1212_loss: 0.0333 - dense_1214_loss: 0.0329 - dense_1216_loss: 0.0308 - dense_1218_loss: 0.0376 - dense_1220_loss: 0.0366 - dense_1222_loss: 0.0437 - dense_1224_loss: 0.0287 - dense_1226_loss: 0.0448 - dense_1228_loss: 0.0268 - dense_1230_loss: 0.0401 - dense_1232_loss: 0.0536 - dense_1234_loss: 0.0392 - dense_1236_loss: 0.0338 - dense_1238_loss: 0.0444 - dense_1240_loss: 0.0469 - dense_1242_loss: 0.0607 - dense_1244_loss: 0.0484 - dense_1246_loss: 0.0397 - dense_1248_loss: 0.0323 - dense_1250_loss: 0.0529 - dense_1252_loss: 0.0525 - dense_1254_loss: 0.0529 - dense_1256_loss: 0.0550 - dense_1258_loss: 0.0753 - dense_1260_loss: 0.0366 - dense_1262_loss: 0.0487 - dense_1264_loss: 0.0475 - dense_1266_loss: 0.0450 - dense_1268_loss: 0.0592 - dense_1270_loss: 0.0593 - dense_1272_loss: 0.0829 - dense_1274_loss: 0.0851 - dense_1276_loss: 0.0348 - dense_1278_loss: 0.1036 - dense_1280_loss: 0.0657 - dense_1282_loss: 0.0649 - dense_1284_loss: 0.0698 - dense_1286_loss: 0.0595 - dense_1288_loss: 0.0654 - dense_1290_loss: 0.0736 - dense_1292_loss: 0.0074 - dense_1294_loss: 0.0515 - dense_1296_loss: 0.0634 - dense_1298_loss: 0.0543 - dense_1300_loss: 0.0815 - dense_1302_loss: 0.0977 - dense_1304_loss: 0.1178 - dense_1306_loss: 0.1174 - dense_1308_loss: 0.0979 - dense_1310_loss: 0.1257 - dense_1312_loss: 0.1863 - dense_1314_loss: 0.1551 - dense_1316_loss: 0.1186 - dense_1318_loss: 0.1623 - dense_1320_loss: 0.1663 - dense_1322_loss: 0.1813 - dense_1324_loss: 0.2189 - dense_1326_loss: 0.3204\n",
      "Epoch 12/30\n",
      " - 39s - loss: 16.8807 - dense_1108_loss: 0.0148 - dense_1110_loss: 0.0142 - dense_1112_loss: 0.0152 - dense_1114_loss: 0.0083 - dense_1116_loss: 0.0172 - dense_1118_loss: 0.0074 - dense_1120_loss: 0.0196 - dense_1122_loss: 0.0148 - dense_1124_loss: 0.0257 - dense_1126_loss: 0.0158 - dense_1128_loss: 0.0139 - dense_1130_loss: 0.0184 - dense_1132_loss: 0.0198 - dense_1134_loss: 0.0299 - dense_1136_loss: 0.0228 - dense_1138_loss: 0.0302 - dense_1140_loss: 0.0145 - dense_1142_loss: 0.0207 - dense_1144_loss: 0.0229 - dense_1146_loss: 0.0341 - dense_1148_loss: 0.0248 - dense_1150_loss: 0.0231 - dense_1152_loss: 0.0269 - dense_1154_loss: 0.0148 - dense_1156_loss: 0.0115 - dense_1158_loss: 0.0176 - dense_1160_loss: 0.0174 - dense_1162_loss: 0.0132 - dense_1164_loss: 0.0168 - dense_1166_loss: 0.0270 - dense_1168_loss: 0.0186 - dense_1170_loss: 0.0199 - dense_1172_loss: 0.0220 - dense_1174_loss: 0.0130 - dense_1176_loss: 0.0323 - dense_1178_loss: 0.0375 - dense_1180_loss: 0.0229 - dense_1182_loss: 0.0379 - dense_1184_loss: 0.0178 - dense_1186_loss: 0.0204 - dense_1188_loss: 0.0469 - dense_1190_loss: 0.0248 - dense_1192_loss: 0.0306 - dense_1194_loss: 0.0161 - dense_1196_loss: 0.0388 - dense_1198_loss: 0.0292 - dense_1200_loss: 0.0168 - dense_1202_loss: 0.0192 - dense_1204_loss: 0.0362 - dense_1206_loss: 0.0232 - dense_1208_loss: 0.0409 - dense_1210_loss: 0.0431 - dense_1212_loss: 0.0355 - dense_1214_loss: 0.0293 - dense_1216_loss: 0.0374 - dense_1218_loss: 0.0294 - dense_1220_loss: 0.0388 - dense_1222_loss: 0.0399 - dense_1224_loss: 0.0192 - dense_1226_loss: 0.0463 - dense_1228_loss: 0.0246 - dense_1230_loss: 0.0394 - dense_1232_loss: 0.0542 - dense_1234_loss: 0.0459 - dense_1236_loss: 0.0267 - dense_1238_loss: 0.0539 - dense_1240_loss: 0.0497 - dense_1242_loss: 0.0542 - dense_1244_loss: 0.0573 - dense_1246_loss: 0.0417 - dense_1248_loss: 0.0322 - dense_1250_loss: 0.0424 - dense_1252_loss: 0.0478 - dense_1254_loss: 0.0660 - dense_1256_loss: 0.0581 - dense_1258_loss: 0.0793 - dense_1260_loss: 0.0325 - dense_1262_loss: 0.0497 - dense_1264_loss: 0.0415 - dense_1266_loss: 0.0546 - dense_1268_loss: 0.0539 - dense_1270_loss: 0.0642 - dense_1272_loss: 0.0745 - dense_1274_loss: 0.0803 - dense_1276_loss: 0.0381 - dense_1278_loss: 0.1091 - dense_1280_loss: 0.0512 - dense_1282_loss: 0.0583 - dense_1284_loss: 0.0659 - dense_1286_loss: 0.0658 - dense_1288_loss: 0.0661 - dense_1290_loss: 0.0772 - dense_1292_loss: 0.0069 - dense_1294_loss: 0.0548 - dense_1296_loss: 0.0588 - dense_1298_loss: 0.0527 - dense_1300_loss: 0.0762 - dense_1302_loss: 0.1168 - dense_1304_loss: 0.1173 - dense_1306_loss: 0.1070 - dense_1308_loss: 0.1008 - dense_1310_loss: 0.1362 - dense_1312_loss: 0.1959 - dense_1314_loss: 0.1220 - dense_1316_loss: 0.1276 - dense_1318_loss: 0.1377 - dense_1320_loss: 0.1534 - dense_1322_loss: 0.1736 - dense_1324_loss: 0.2141 - dense_1326_loss: 0.3226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30\n",
      " - 39s - loss: 15.8076 - dense_1108_loss: 0.0154 - dense_1110_loss: 0.0132 - dense_1112_loss: 0.0136 - dense_1114_loss: 0.0082 - dense_1116_loss: 0.0140 - dense_1118_loss: 0.0078 - dense_1120_loss: 0.0176 - dense_1122_loss: 0.0156 - dense_1124_loss: 0.0280 - dense_1126_loss: 0.0143 - dense_1128_loss: 0.0150 - dense_1130_loss: 0.0167 - dense_1132_loss: 0.0161 - dense_1134_loss: 0.0249 - dense_1136_loss: 0.0204 - dense_1138_loss: 0.0166 - dense_1140_loss: 0.0157 - dense_1142_loss: 0.0143 - dense_1144_loss: 0.0200 - dense_1146_loss: 0.0274 - dense_1148_loss: 0.0224 - dense_1150_loss: 0.0187 - dense_1152_loss: 0.0289 - dense_1154_loss: 0.0144 - dense_1156_loss: 0.0140 - dense_1158_loss: 0.0193 - dense_1160_loss: 0.0185 - dense_1162_loss: 0.0153 - dense_1164_loss: 0.0169 - dense_1166_loss: 0.0159 - dense_1168_loss: 0.0160 - dense_1170_loss: 0.0193 - dense_1172_loss: 0.0255 - dense_1174_loss: 0.0140 - dense_1176_loss: 0.0244 - dense_1178_loss: 0.0304 - dense_1180_loss: 0.0232 - dense_1182_loss: 0.0299 - dense_1184_loss: 0.0170 - dense_1186_loss: 0.0197 - dense_1188_loss: 0.0410 - dense_1190_loss: 0.0282 - dense_1192_loss: 0.0268 - dense_1194_loss: 0.0168 - dense_1196_loss: 0.0339 - dense_1198_loss: 0.0290 - dense_1200_loss: 0.0149 - dense_1202_loss: 0.0187 - dense_1204_loss: 0.0320 - dense_1206_loss: 0.0304 - dense_1208_loss: 0.0399 - dense_1210_loss: 0.0356 - dense_1212_loss: 0.0306 - dense_1214_loss: 0.0348 - dense_1216_loss: 0.0307 - dense_1218_loss: 0.0265 - dense_1220_loss: 0.0357 - dense_1222_loss: 0.0392 - dense_1224_loss: 0.0229 - dense_1226_loss: 0.0425 - dense_1228_loss: 0.0247 - dense_1230_loss: 0.0509 - dense_1232_loss: 0.0421 - dense_1234_loss: 0.0369 - dense_1236_loss: 0.0271 - dense_1238_loss: 0.0413 - dense_1240_loss: 0.0431 - dense_1242_loss: 0.0410 - dense_1244_loss: 0.0505 - dense_1246_loss: 0.0402 - dense_1248_loss: 0.0327 - dense_1250_loss: 0.0477 - dense_1252_loss: 0.0515 - dense_1254_loss: 0.0562 - dense_1256_loss: 0.0490 - dense_1258_loss: 0.0696 - dense_1260_loss: 0.0368 - dense_1262_loss: 0.0452 - dense_1264_loss: 0.0547 - dense_1266_loss: 0.0423 - dense_1268_loss: 0.0519 - dense_1270_loss: 0.0569 - dense_1272_loss: 0.0636 - dense_1274_loss: 0.0825 - dense_1276_loss: 0.0372 - dense_1278_loss: 0.1015 - dense_1280_loss: 0.0658 - dense_1282_loss: 0.0567 - dense_1284_loss: 0.0657 - dense_1286_loss: 0.0622 - dense_1288_loss: 0.0567 - dense_1290_loss: 0.0719 - dense_1292_loss: 0.0071 - dense_1294_loss: 0.0805 - dense_1296_loss: 0.0717 - dense_1298_loss: 0.0488 - dense_1300_loss: 0.0745 - dense_1302_loss: 0.0975 - dense_1304_loss: 0.1193 - dense_1306_loss: 0.1163 - dense_1308_loss: 0.0958 - dense_1310_loss: 0.1334 - dense_1312_loss: 0.1719 - dense_1314_loss: 0.1300 - dense_1316_loss: 0.1234 - dense_1318_loss: 0.1453 - dense_1320_loss: 0.1434 - dense_1322_loss: 0.1746 - dense_1324_loss: 0.2110 - dense_1326_loss: 0.3139\n",
      "Epoch 14/30\n",
      " - 40s - loss: 14.8957 - dense_1108_loss: 0.0136 - dense_1110_loss: 0.0139 - dense_1112_loss: 0.0147 - dense_1114_loss: 0.0107 - dense_1116_loss: 0.0153 - dense_1118_loss: 0.0079 - dense_1120_loss: 0.0173 - dense_1122_loss: 0.0176 - dense_1124_loss: 0.0227 - dense_1126_loss: 0.0107 - dense_1128_loss: 0.0134 - dense_1130_loss: 0.0186 - dense_1132_loss: 0.0166 - dense_1134_loss: 0.0223 - dense_1136_loss: 0.0210 - dense_1138_loss: 0.0181 - dense_1140_loss: 0.0137 - dense_1142_loss: 0.0158 - dense_1144_loss: 0.0202 - dense_1146_loss: 0.0270 - dense_1148_loss: 0.0234 - dense_1150_loss: 0.0220 - dense_1152_loss: 0.0283 - dense_1154_loss: 0.0178 - dense_1156_loss: 0.0124 - dense_1158_loss: 0.0178 - dense_1160_loss: 0.0156 - dense_1162_loss: 0.0166 - dense_1164_loss: 0.0177 - dense_1166_loss: 0.0169 - dense_1168_loss: 0.0142 - dense_1170_loss: 0.0176 - dense_1172_loss: 0.0231 - dense_1174_loss: 0.0121 - dense_1176_loss: 0.0289 - dense_1178_loss: 0.0304 - dense_1180_loss: 0.0198 - dense_1182_loss: 0.0339 - dense_1184_loss: 0.0147 - dense_1186_loss: 0.0176 - dense_1188_loss: 0.0426 - dense_1190_loss: 0.0214 - dense_1192_loss: 0.0292 - dense_1194_loss: 0.0149 - dense_1196_loss: 0.0309 - dense_1198_loss: 0.0271 - dense_1200_loss: 0.0151 - dense_1202_loss: 0.0151 - dense_1204_loss: 0.0260 - dense_1206_loss: 0.0234 - dense_1208_loss: 0.0370 - dense_1210_loss: 0.0344 - dense_1212_loss: 0.0266 - dense_1214_loss: 0.0283 - dense_1216_loss: 0.0318 - dense_1218_loss: 0.0264 - dense_1220_loss: 0.0394 - dense_1222_loss: 0.0400 - dense_1224_loss: 0.0202 - dense_1226_loss: 0.0385 - dense_1228_loss: 0.0240 - dense_1230_loss: 0.0457 - dense_1232_loss: 0.0441 - dense_1234_loss: 0.0394 - dense_1236_loss: 0.0297 - dense_1238_loss: 0.0388 - dense_1240_loss: 0.0411 - dense_1242_loss: 0.0314 - dense_1244_loss: 0.0553 - dense_1246_loss: 0.0394 - dense_1248_loss: 0.0286 - dense_1250_loss: 0.0391 - dense_1252_loss: 0.0417 - dense_1254_loss: 0.0499 - dense_1256_loss: 0.0427 - dense_1258_loss: 0.0667 - dense_1260_loss: 0.0362 - dense_1262_loss: 0.0437 - dense_1264_loss: 0.0377 - dense_1266_loss: 0.0385 - dense_1268_loss: 0.0539 - dense_1270_loss: 0.0582 - dense_1272_loss: 0.0586 - dense_1274_loss: 0.0885 - dense_1276_loss: 0.0400 - dense_1278_loss: 0.1028 - dense_1280_loss: 0.0568 - dense_1282_loss: 0.0555 - dense_1284_loss: 0.0640 - dense_1286_loss: 0.0594 - dense_1288_loss: 0.0506 - dense_1290_loss: 0.0727 - dense_1292_loss: 0.0076 - dense_1294_loss: 0.0536 - dense_1296_loss: 0.0614 - dense_1298_loss: 0.0554 - dense_1300_loss: 0.0725 - dense_1302_loss: 0.0845 - dense_1304_loss: 0.1110 - dense_1306_loss: 0.1099 - dense_1308_loss: 0.0963 - dense_1310_loss: 0.1263 - dense_1312_loss: 0.1689 - dense_1314_loss: 0.1242 - dense_1316_loss: 0.1248 - dense_1318_loss: 0.1446 - dense_1320_loss: 0.1622 - dense_1322_loss: 0.1741 - dense_1324_loss: 0.2166 - dense_1326_loss: 0.3172\n",
      "Epoch 15/30\n",
      " - 40s - loss: 14.1296 - dense_1108_loss: 0.0159 - dense_1110_loss: 0.0118 - dense_1112_loss: 0.0110 - dense_1114_loss: 0.0092 - dense_1116_loss: 0.0132 - dense_1118_loss: 0.0102 - dense_1120_loss: 0.0139 - dense_1122_loss: 0.0139 - dense_1124_loss: 0.0162 - dense_1126_loss: 0.0111 - dense_1128_loss: 0.0145 - dense_1130_loss: 0.0147 - dense_1132_loss: 0.0196 - dense_1134_loss: 0.0193 - dense_1136_loss: 0.0231 - dense_1138_loss: 0.0141 - dense_1140_loss: 0.0150 - dense_1142_loss: 0.0148 - dense_1144_loss: 0.0194 - dense_1146_loss: 0.0199 - dense_1148_loss: 0.0219 - dense_1150_loss: 0.0218 - dense_1152_loss: 0.0256 - dense_1154_loss: 0.0166 - dense_1156_loss: 0.0146 - dense_1158_loss: 0.0162 - dense_1160_loss: 0.0174 - dense_1162_loss: 0.0160 - dense_1164_loss: 0.0167 - dense_1166_loss: 0.0155 - dense_1168_loss: 0.0136 - dense_1170_loss: 0.0152 - dense_1172_loss: 0.0214 - dense_1174_loss: 0.0117 - dense_1176_loss: 0.0346 - dense_1178_loss: 0.0345 - dense_1180_loss: 0.0205 - dense_1182_loss: 0.0293 - dense_1184_loss: 0.0162 - dense_1186_loss: 0.0185 - dense_1188_loss: 0.0329 - dense_1190_loss: 0.0254 - dense_1192_loss: 0.0299 - dense_1194_loss: 0.0137 - dense_1196_loss: 0.0281 - dense_1198_loss: 0.0245 - dense_1200_loss: 0.0167 - dense_1202_loss: 0.0150 - dense_1204_loss: 0.0238 - dense_1206_loss: 0.0218 - dense_1208_loss: 0.0385 - dense_1210_loss: 0.0328 - dense_1212_loss: 0.0260 - dense_1214_loss: 0.0271 - dense_1216_loss: 0.0299 - dense_1218_loss: 0.0198 - dense_1220_loss: 0.0421 - dense_1222_loss: 0.0381 - dense_1224_loss: 0.0182 - dense_1226_loss: 0.0344 - dense_1228_loss: 0.0232 - dense_1230_loss: 0.0438 - dense_1232_loss: 0.0381 - dense_1234_loss: 0.0348 - dense_1236_loss: 0.0241 - dense_1238_loss: 0.0425 - dense_1240_loss: 0.0421 - dense_1242_loss: 0.0321 - dense_1244_loss: 0.0548 - dense_1246_loss: 0.0365 - dense_1248_loss: 0.0306 - dense_1250_loss: 0.0424 - dense_1252_loss: 0.0425 - dense_1254_loss: 0.0478 - dense_1256_loss: 0.0461 - dense_1258_loss: 0.0543 - dense_1260_loss: 0.0323 - dense_1262_loss: 0.0462 - dense_1264_loss: 0.0413 - dense_1266_loss: 0.0387 - dense_1268_loss: 0.0484 - dense_1270_loss: 0.0450 - dense_1272_loss: 0.0503 - dense_1274_loss: 0.0775 - dense_1276_loss: 0.0363 - dense_1278_loss: 0.0906 - dense_1280_loss: 0.0494 - dense_1282_loss: 0.0527 - dense_1284_loss: 0.0667 - dense_1286_loss: 0.0551 - dense_1288_loss: 0.0513 - dense_1290_loss: 0.0741 - dense_1292_loss: 0.0071 - dense_1294_loss: 0.0543 - dense_1296_loss: 0.0681 - dense_1298_loss: 0.0451 - dense_1300_loss: 0.0730 - dense_1302_loss: 0.0886 - dense_1304_loss: 0.1131 - dense_1306_loss: 0.1023 - dense_1308_loss: 0.0936 - dense_1310_loss: 0.1146 - dense_1312_loss: 0.1414 - dense_1314_loss: 0.1173 - dense_1316_loss: 0.1170 - dense_1318_loss: 0.1429 - dense_1320_loss: 0.1536 - dense_1322_loss: 0.1853 - dense_1324_loss: 0.2057 - dense_1326_loss: 0.2890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30\n",
      " - 39s - loss: 13.7891 - dense_1108_loss: 0.0155 - dense_1110_loss: 0.0144 - dense_1112_loss: 0.0124 - dense_1114_loss: 0.0126 - dense_1116_loss: 0.0169 - dense_1118_loss: 0.0065 - dense_1120_loss: 0.0155 - dense_1122_loss: 0.0148 - dense_1124_loss: 0.0168 - dense_1126_loss: 0.0150 - dense_1128_loss: 0.0138 - dense_1130_loss: 0.0164 - dense_1132_loss: 0.0128 - dense_1134_loss: 0.0208 - dense_1136_loss: 0.0208 - dense_1138_loss: 0.0182 - dense_1140_loss: 0.0152 - dense_1142_loss: 0.0161 - dense_1144_loss: 0.0163 - dense_1146_loss: 0.0206 - dense_1148_loss: 0.0161 - dense_1150_loss: 0.0210 - dense_1152_loss: 0.0232 - dense_1154_loss: 0.0150 - dense_1156_loss: 0.0119 - dense_1158_loss: 0.0177 - dense_1160_loss: 0.0167 - dense_1162_loss: 0.0148 - dense_1164_loss: 0.0175 - dense_1166_loss: 0.0140 - dense_1168_loss: 0.0102 - dense_1170_loss: 0.0152 - dense_1172_loss: 0.0191 - dense_1174_loss: 0.0136 - dense_1176_loss: 0.0235 - dense_1178_loss: 0.0293 - dense_1180_loss: 0.0209 - dense_1182_loss: 0.0332 - dense_1184_loss: 0.0158 - dense_1186_loss: 0.0141 - dense_1188_loss: 0.0349 - dense_1190_loss: 0.0226 - dense_1192_loss: 0.0253 - dense_1194_loss: 0.0156 - dense_1196_loss: 0.0222 - dense_1198_loss: 0.0188 - dense_1200_loss: 0.0137 - dense_1202_loss: 0.0129 - dense_1204_loss: 0.0255 - dense_1206_loss: 0.0240 - dense_1208_loss: 0.0384 - dense_1210_loss: 0.0387 - dense_1212_loss: 0.0239 - dense_1214_loss: 0.0294 - dense_1216_loss: 0.0340 - dense_1218_loss: 0.0205 - dense_1220_loss: 0.0383 - dense_1222_loss: 0.0382 - dense_1224_loss: 0.0200 - dense_1226_loss: 0.0321 - dense_1228_loss: 0.0228 - dense_1230_loss: 0.0466 - dense_1232_loss: 0.0371 - dense_1234_loss: 0.0370 - dense_1236_loss: 0.0235 - dense_1238_loss: 0.0404 - dense_1240_loss: 0.0408 - dense_1242_loss: 0.0288 - dense_1244_loss: 0.0583 - dense_1246_loss: 0.0362 - dense_1248_loss: 0.0265 - dense_1250_loss: 0.0406 - dense_1252_loss: 0.0398 - dense_1254_loss: 0.0621 - dense_1256_loss: 0.0482 - dense_1258_loss: 0.0598 - dense_1260_loss: 0.0371 - dense_1262_loss: 0.0498 - dense_1264_loss: 0.0390 - dense_1266_loss: 0.0417 - dense_1268_loss: 0.0597 - dense_1270_loss: 0.0510 - dense_1272_loss: 0.0499 - dense_1274_loss: 0.0813 - dense_1276_loss: 0.0393 - dense_1278_loss: 0.0835 - dense_1280_loss: 0.0485 - dense_1282_loss: 0.0541 - dense_1284_loss: 0.0604 - dense_1286_loss: 0.0531 - dense_1288_loss: 0.0460 - dense_1290_loss: 0.0730 - dense_1292_loss: 0.0069 - dense_1294_loss: 0.0609 - dense_1296_loss: 0.0734 - dense_1298_loss: 0.0539 - dense_1300_loss: 0.0673 - dense_1302_loss: 0.0860 - dense_1304_loss: 0.1110 - dense_1306_loss: 0.1193 - dense_1308_loss: 0.0999 - dense_1310_loss: 0.1225 - dense_1312_loss: 0.1518 - dense_1314_loss: 0.1056 - dense_1316_loss: 0.1160 - dense_1318_loss: 0.1267 - dense_1320_loss: 0.1496 - dense_1322_loss: 0.1592 - dense_1324_loss: 0.2122 - dense_1326_loss: 0.3182\n",
      "Epoch 17/30\n",
      " - 38s - loss: 13.2681 - dense_1108_loss: 0.0122 - dense_1110_loss: 0.0142 - dense_1112_loss: 0.0130 - dense_1114_loss: 0.0096 - dense_1116_loss: 0.0153 - dense_1118_loss: 0.0200 - dense_1120_loss: 0.0139 - dense_1122_loss: 0.0158 - dense_1124_loss: 0.0159 - dense_1126_loss: 0.0109 - dense_1128_loss: 0.0126 - dense_1130_loss: 0.0163 - dense_1132_loss: 0.0158 - dense_1134_loss: 0.0198 - dense_1136_loss: 0.0179 - dense_1138_loss: 0.0213 - dense_1140_loss: 0.0127 - dense_1142_loss: 0.0121 - dense_1144_loss: 0.0157 - dense_1146_loss: 0.0177 - dense_1148_loss: 0.0165 - dense_1150_loss: 0.0189 - dense_1152_loss: 0.0252 - dense_1154_loss: 0.0130 - dense_1156_loss: 0.0111 - dense_1158_loss: 0.0256 - dense_1160_loss: 0.0148 - dense_1162_loss: 0.0134 - dense_1164_loss: 0.0130 - dense_1166_loss: 0.0136 - dense_1168_loss: 0.0118 - dense_1170_loss: 0.0146 - dense_1172_loss: 0.0164 - dense_1174_loss: 0.0132 - dense_1176_loss: 0.0205 - dense_1178_loss: 0.0316 - dense_1180_loss: 0.0220 - dense_1182_loss: 0.0234 - dense_1184_loss: 0.0150 - dense_1186_loss: 0.0140 - dense_1188_loss: 0.0290 - dense_1190_loss: 0.0221 - dense_1192_loss: 0.0183 - dense_1194_loss: 0.0141 - dense_1196_loss: 0.0262 - dense_1198_loss: 0.0189 - dense_1200_loss: 0.0121 - dense_1202_loss: 0.0143 - dense_1204_loss: 0.0209 - dense_1206_loss: 0.0183 - dense_1208_loss: 0.0376 - dense_1210_loss: 0.0323 - dense_1212_loss: 0.0245 - dense_1214_loss: 0.0280 - dense_1216_loss: 0.0282 - dense_1218_loss: 0.0197 - dense_1220_loss: 0.0380 - dense_1222_loss: 0.0338 - dense_1224_loss: 0.0203 - dense_1226_loss: 0.0326 - dense_1228_loss: 0.0210 - dense_1230_loss: 0.0354 - dense_1232_loss: 0.0360 - dense_1234_loss: 0.0358 - dense_1236_loss: 0.0245 - dense_1238_loss: 0.0469 - dense_1240_loss: 0.0405 - dense_1242_loss: 0.0264 - dense_1244_loss: 0.0452 - dense_1246_loss: 0.0352 - dense_1248_loss: 0.0232 - dense_1250_loss: 0.0413 - dense_1252_loss: 0.0423 - dense_1254_loss: 0.0424 - dense_1256_loss: 0.0380 - dense_1258_loss: 0.0546 - dense_1260_loss: 0.0313 - dense_1262_loss: 0.0437 - dense_1264_loss: 0.0435 - dense_1266_loss: 0.0343 - dense_1268_loss: 0.0534 - dense_1270_loss: 0.0467 - dense_1272_loss: 0.0436 - dense_1274_loss: 0.0787 - dense_1276_loss: 0.0327 - dense_1278_loss: 0.0857 - dense_1280_loss: 0.0469 - dense_1282_loss: 0.0481 - dense_1284_loss: 0.0606 - dense_1286_loss: 0.0490 - dense_1288_loss: 0.0509 - dense_1290_loss: 0.0605 - dense_1292_loss: 0.0072 - dense_1294_loss: 0.0607 - dense_1296_loss: 0.0973 - dense_1298_loss: 0.0445 - dense_1300_loss: 0.0720 - dense_1302_loss: 0.0795 - dense_1304_loss: 0.0991 - dense_1306_loss: 0.1089 - dense_1308_loss: 0.0847 - dense_1310_loss: 0.1034 - dense_1312_loss: 0.1425 - dense_1314_loss: 0.1284 - dense_1316_loss: 0.1276 - dense_1318_loss: 0.1548 - dense_1320_loss: 0.1440 - dense_1322_loss: 0.1757 - dense_1324_loss: 0.2190 - dense_1326_loss: 0.3042\n",
      "Epoch 18/30\n",
      " - 41s - loss: 12.8989 - dense_1108_loss: 0.0173 - dense_1110_loss: 0.0106 - dense_1112_loss: 0.0114 - dense_1114_loss: 0.0103 - dense_1116_loss: 0.0140 - dense_1118_loss: 0.0060 - dense_1120_loss: 0.0133 - dense_1122_loss: 0.0197 - dense_1124_loss: 0.0166 - dense_1126_loss: 0.0090 - dense_1128_loss: 0.0126 - dense_1130_loss: 0.0128 - dense_1132_loss: 0.0130 - dense_1134_loss: 0.0130 - dense_1136_loss: 0.0198 - dense_1138_loss: 0.0165 - dense_1140_loss: 0.0146 - dense_1142_loss: 0.0144 - dense_1144_loss: 0.0155 - dense_1146_loss: 0.0190 - dense_1148_loss: 0.0126 - dense_1150_loss: 0.0209 - dense_1152_loss: 0.0275 - dense_1154_loss: 0.0187 - dense_1156_loss: 0.0119 - dense_1158_loss: 0.0211 - dense_1160_loss: 0.0181 - dense_1162_loss: 0.0110 - dense_1164_loss: 0.0125 - dense_1166_loss: 0.0130 - dense_1168_loss: 0.0114 - dense_1170_loss: 0.0145 - dense_1172_loss: 0.0177 - dense_1174_loss: 0.0147 - dense_1176_loss: 0.0315 - dense_1178_loss: 0.0285 - dense_1180_loss: 0.0161 - dense_1182_loss: 0.0237 - dense_1184_loss: 0.0139 - dense_1186_loss: 0.0143 - dense_1188_loss: 0.0261 - dense_1190_loss: 0.0226 - dense_1192_loss: 0.0193 - dense_1194_loss: 0.0110 - dense_1196_loss: 0.0232 - dense_1198_loss: 0.0166 - dense_1200_loss: 0.0136 - dense_1202_loss: 0.0133 - dense_1204_loss: 0.0205 - dense_1206_loss: 0.0190 - dense_1208_loss: 0.0370 - dense_1210_loss: 0.0291 - dense_1212_loss: 0.0192 - dense_1214_loss: 0.0264 - dense_1216_loss: 0.0267 - dense_1218_loss: 0.0202 - dense_1220_loss: 0.0347 - dense_1222_loss: 0.0343 - dense_1224_loss: 0.0191 - dense_1226_loss: 0.0314 - dense_1228_loss: 0.0246 - dense_1230_loss: 0.0430 - dense_1232_loss: 0.0340 - dense_1234_loss: 0.0346 - dense_1236_loss: 0.0275 - dense_1238_loss: 0.0381 - dense_1240_loss: 0.0346 - dense_1242_loss: 0.0319 - dense_1244_loss: 0.0519 - dense_1246_loss: 0.0333 - dense_1248_loss: 0.0283 - dense_1250_loss: 0.0372 - dense_1252_loss: 0.0446 - dense_1254_loss: 0.0401 - dense_1256_loss: 0.0367 - dense_1258_loss: 0.0496 - dense_1260_loss: 0.0292 - dense_1262_loss: 0.0396 - dense_1264_loss: 0.0388 - dense_1266_loss: 0.0374 - dense_1268_loss: 0.0647 - dense_1270_loss: 0.0414 - dense_1272_loss: 0.0431 - dense_1274_loss: 0.0698 - dense_1276_loss: 0.0367 - dense_1278_loss: 0.0733 - dense_1280_loss: 0.0548 - dense_1282_loss: 0.0481 - dense_1284_loss: 0.0614 - dense_1286_loss: 0.0494 - dense_1288_loss: 0.0437 - dense_1290_loss: 0.0684 - dense_1292_loss: 0.0067 - dense_1294_loss: 0.0501 - dense_1296_loss: 0.0792 - dense_1298_loss: 0.0489 - dense_1300_loss: 0.0672 - dense_1302_loss: 0.0836 - dense_1304_loss: 0.0958 - dense_1306_loss: 0.1018 - dense_1308_loss: 0.0968 - dense_1310_loss: 0.1051 - dense_1312_loss: 0.1395 - dense_1314_loss: 0.1043 - dense_1316_loss: 0.1141 - dense_1318_loss: 0.1456 - dense_1320_loss: 0.1455 - dense_1322_loss: 0.1773 - dense_1324_loss: 0.2002 - dense_1326_loss: 0.3011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      " - 39s - loss: 12.6577 - dense_1108_loss: 0.0142 - dense_1110_loss: 0.0122 - dense_1112_loss: 0.0105 - dense_1114_loss: 0.0084 - dense_1116_loss: 0.0122 - dense_1118_loss: 0.0074 - dense_1120_loss: 0.0085 - dense_1122_loss: 0.0127 - dense_1124_loss: 0.0147 - dense_1126_loss: 0.0083 - dense_1128_loss: 0.0129 - dense_1130_loss: 0.0171 - dense_1132_loss: 0.0132 - dense_1134_loss: 0.0174 - dense_1136_loss: 0.0167 - dense_1138_loss: 0.0157 - dense_1140_loss: 0.0142 - dense_1142_loss: 0.0132 - dense_1144_loss: 0.0162 - dense_1146_loss: 0.0198 - dense_1148_loss: 0.0143 - dense_1150_loss: 0.0187 - dense_1152_loss: 0.0297 - dense_1154_loss: 0.0120 - dense_1156_loss: 0.0104 - dense_1158_loss: 0.0201 - dense_1160_loss: 0.0156 - dense_1162_loss: 0.0136 - dense_1164_loss: 0.0128 - dense_1166_loss: 0.0139 - dense_1168_loss: 0.0117 - dense_1170_loss: 0.0130 - dense_1172_loss: 0.0192 - dense_1174_loss: 0.0146 - dense_1176_loss: 0.0302 - dense_1178_loss: 0.0261 - dense_1180_loss: 0.0172 - dense_1182_loss: 0.0207 - dense_1184_loss: 0.0140 - dense_1186_loss: 0.0151 - dense_1188_loss: 0.0244 - dense_1190_loss: 0.0214 - dense_1192_loss: 0.0209 - dense_1194_loss: 0.0174 - dense_1196_loss: 0.0206 - dense_1198_loss: 0.0202 - dense_1200_loss: 0.0112 - dense_1202_loss: 0.0180 - dense_1204_loss: 0.0246 - dense_1206_loss: 0.0189 - dense_1208_loss: 0.0328 - dense_1210_loss: 0.0315 - dense_1212_loss: 0.0230 - dense_1214_loss: 0.0244 - dense_1216_loss: 0.0355 - dense_1218_loss: 0.0216 - dense_1220_loss: 0.0428 - dense_1222_loss: 0.0322 - dense_1224_loss: 0.0191 - dense_1226_loss: 0.0289 - dense_1228_loss: 0.0252 - dense_1230_loss: 0.0384 - dense_1232_loss: 0.0336 - dense_1234_loss: 0.0335 - dense_1236_loss: 0.0292 - dense_1238_loss: 0.0379 - dense_1240_loss: 0.0364 - dense_1242_loss: 0.0275 - dense_1244_loss: 0.0467 - dense_1246_loss: 0.0326 - dense_1248_loss: 0.0277 - dense_1250_loss: 0.0421 - dense_1252_loss: 0.0391 - dense_1254_loss: 0.0436 - dense_1256_loss: 0.0341 - dense_1258_loss: 0.0512 - dense_1260_loss: 0.0332 - dense_1262_loss: 0.0419 - dense_1264_loss: 0.0375 - dense_1266_loss: 0.0373 - dense_1268_loss: 0.0526 - dense_1270_loss: 0.0433 - dense_1272_loss: 0.0467 - dense_1274_loss: 0.0697 - dense_1276_loss: 0.0349 - dense_1278_loss: 0.0707 - dense_1280_loss: 0.0434 - dense_1282_loss: 0.0548 - dense_1284_loss: 0.0593 - dense_1286_loss: 0.0443 - dense_1288_loss: 0.0479 - dense_1290_loss: 0.0609 - dense_1292_loss: 0.0071 - dense_1294_loss: 0.0490 - dense_1296_loss: 0.0733 - dense_1298_loss: 0.0431 - dense_1300_loss: 0.0610 - dense_1302_loss: 0.0764 - dense_1304_loss: 0.1168 - dense_1306_loss: 0.1032 - dense_1308_loss: 0.0968 - dense_1310_loss: 0.1019 - dense_1312_loss: 0.1403 - dense_1314_loss: 0.1188 - dense_1316_loss: 0.1069 - dense_1318_loss: 0.1426 - dense_1320_loss: 0.1530 - dense_1322_loss: 0.1508 - dense_1324_loss: 0.2190 - dense_1326_loss: 0.3024\n",
      "Epoch 20/30\n",
      " - 40s - loss: 12.3776 - dense_1108_loss: 0.0160 - dense_1110_loss: 0.0087 - dense_1112_loss: 0.0092 - dense_1114_loss: 0.0101 - dense_1116_loss: 0.0117 - dense_1118_loss: 0.0061 - dense_1120_loss: 0.0124 - dense_1122_loss: 0.0140 - dense_1124_loss: 0.0132 - dense_1126_loss: 0.0064 - dense_1128_loss: 0.0120 - dense_1130_loss: 0.0150 - dense_1132_loss: 0.0134 - dense_1134_loss: 0.0176 - dense_1136_loss: 0.0185 - dense_1138_loss: 0.0125 - dense_1140_loss: 0.0127 - dense_1142_loss: 0.0121 - dense_1144_loss: 0.0163 - dense_1146_loss: 0.0160 - dense_1148_loss: 0.0137 - dense_1150_loss: 0.0291 - dense_1152_loss: 0.0192 - dense_1154_loss: 0.0196 - dense_1156_loss: 0.0086 - dense_1158_loss: 0.0188 - dense_1160_loss: 0.0167 - dense_1162_loss: 0.0127 - dense_1164_loss: 0.0103 - dense_1166_loss: 0.0134 - dense_1168_loss: 0.0110 - dense_1170_loss: 0.0153 - dense_1172_loss: 0.0169 - dense_1174_loss: 0.0111 - dense_1176_loss: 0.0215 - dense_1178_loss: 0.0237 - dense_1180_loss: 0.0165 - dense_1182_loss: 0.0284 - dense_1184_loss: 0.0137 - dense_1186_loss: 0.0143 - dense_1188_loss: 0.0248 - dense_1190_loss: 0.0216 - dense_1192_loss: 0.0211 - dense_1194_loss: 0.0128 - dense_1196_loss: 0.0199 - dense_1198_loss: 0.0174 - dense_1200_loss: 0.0143 - dense_1202_loss: 0.0135 - dense_1204_loss: 0.0171 - dense_1206_loss: 0.0193 - dense_1208_loss: 0.0338 - dense_1210_loss: 0.0277 - dense_1212_loss: 0.0218 - dense_1214_loss: 0.0255 - dense_1216_loss: 0.0283 - dense_1218_loss: 0.0222 - dense_1220_loss: 0.0409 - dense_1222_loss: 0.0309 - dense_1224_loss: 0.0224 - dense_1226_loss: 0.0266 - dense_1228_loss: 0.0229 - dense_1230_loss: 0.0351 - dense_1232_loss: 0.0303 - dense_1234_loss: 0.0308 - dense_1236_loss: 0.0221 - dense_1238_loss: 0.0331 - dense_1240_loss: 0.0374 - dense_1242_loss: 0.0277 - dense_1244_loss: 0.0427 - dense_1246_loss: 0.0315 - dense_1248_loss: 0.0306 - dense_1250_loss: 0.0396 - dense_1252_loss: 0.0412 - dense_1254_loss: 0.0414 - dense_1256_loss: 0.0300 - dense_1258_loss: 0.0452 - dense_1260_loss: 0.0304 - dense_1262_loss: 0.0378 - dense_1264_loss: 0.0410 - dense_1266_loss: 0.0357 - dense_1268_loss: 0.0537 - dense_1270_loss: 0.0416 - dense_1272_loss: 0.0460 - dense_1274_loss: 0.0655 - dense_1276_loss: 0.0424 - dense_1278_loss: 0.0705 - dense_1280_loss: 0.0474 - dense_1282_loss: 0.0536 - dense_1284_loss: 0.0636 - dense_1286_loss: 0.0449 - dense_1288_loss: 0.0506 - dense_1290_loss: 0.0668 - dense_1292_loss: 0.0070 - dense_1294_loss: 0.0628 - dense_1296_loss: 0.0478 - dense_1298_loss: 0.0476 - dense_1300_loss: 0.0612 - dense_1302_loss: 0.0763 - dense_1304_loss: 0.1179 - dense_1306_loss: 0.1006 - dense_1308_loss: 0.0820 - dense_1310_loss: 0.1109 - dense_1312_loss: 0.1380 - dense_1314_loss: 0.1103 - dense_1316_loss: 0.1300 - dense_1318_loss: 0.1328 - dense_1320_loss: 0.1404 - dense_1322_loss: 0.1788 - dense_1324_loss: 0.1962 - dense_1326_loss: 0.3017\n",
      "Epoch 21/30\n",
      " - 39s - loss: 12.2702 - dense_1108_loss: 0.0123 - dense_1110_loss: 0.0098 - dense_1112_loss: 0.0081 - dense_1114_loss: 0.0081 - dense_1116_loss: 0.0162 - dense_1118_loss: 0.0061 - dense_1120_loss: 0.0100 - dense_1122_loss: 0.0171 - dense_1124_loss: 0.0122 - dense_1126_loss: 0.0068 - dense_1128_loss: 0.0116 - dense_1130_loss: 0.0155 - dense_1132_loss: 0.0167 - dense_1134_loss: 0.0160 - dense_1136_loss: 0.0161 - dense_1138_loss: 0.0168 - dense_1140_loss: 0.0114 - dense_1142_loss: 0.0116 - dense_1144_loss: 0.0133 - dense_1146_loss: 0.0144 - dense_1148_loss: 0.0122 - dense_1150_loss: 0.0169 - dense_1152_loss: 0.0253 - dense_1154_loss: 0.0204 - dense_1156_loss: 0.0142 - dense_1158_loss: 0.0161 - dense_1160_loss: 0.0164 - dense_1162_loss: 0.0149 - dense_1164_loss: 0.0108 - dense_1166_loss: 0.0096 - dense_1168_loss: 0.0122 - dense_1170_loss: 0.0147 - dense_1172_loss: 0.0227 - dense_1174_loss: 0.0111 - dense_1176_loss: 0.0252 - dense_1178_loss: 0.0199 - dense_1180_loss: 0.0170 - dense_1182_loss: 0.0233 - dense_1184_loss: 0.0148 - dense_1186_loss: 0.0140 - dense_1188_loss: 0.0225 - dense_1190_loss: 0.0251 - dense_1192_loss: 0.0179 - dense_1194_loss: 0.0151 - dense_1196_loss: 0.0236 - dense_1198_loss: 0.0194 - dense_1200_loss: 0.0132 - dense_1202_loss: 0.0139 - dense_1204_loss: 0.0204 - dense_1206_loss: 0.0189 - dense_1208_loss: 0.0318 - dense_1210_loss: 0.0247 - dense_1212_loss: 0.0192 - dense_1214_loss: 0.0237 - dense_1216_loss: 0.0248 - dense_1218_loss: 0.0216 - dense_1220_loss: 0.0341 - dense_1222_loss: 0.0296 - dense_1224_loss: 0.0182 - dense_1226_loss: 0.0289 - dense_1228_loss: 0.0217 - dense_1230_loss: 0.0397 - dense_1232_loss: 0.0340 - dense_1234_loss: 0.0322 - dense_1236_loss: 0.0241 - dense_1238_loss: 0.0314 - dense_1240_loss: 0.0360 - dense_1242_loss: 0.0267 - dense_1244_loss: 0.0495 - dense_1246_loss: 0.0340 - dense_1248_loss: 0.0250 - dense_1250_loss: 0.0350 - dense_1252_loss: 0.0401 - dense_1254_loss: 0.0453 - dense_1256_loss: 0.0338 - dense_1258_loss: 0.0518 - dense_1260_loss: 0.0354 - dense_1262_loss: 0.0399 - dense_1264_loss: 0.0359 - dense_1266_loss: 0.0350 - dense_1268_loss: 0.0563 - dense_1270_loss: 0.0413 - dense_1272_loss: 0.0533 - dense_1274_loss: 0.0713 - dense_1276_loss: 0.0303 - dense_1278_loss: 0.0751 - dense_1280_loss: 0.0431 - dense_1282_loss: 0.0535 - dense_1284_loss: 0.0579 - dense_1286_loss: 0.0502 - dense_1288_loss: 0.0488 - dense_1290_loss: 0.0639 - dense_1292_loss: 0.0067 - dense_1294_loss: 0.0563 - dense_1296_loss: 0.0485 - dense_1298_loss: 0.0504 - dense_1300_loss: 0.0619 - dense_1302_loss: 0.0849 - dense_1304_loss: 0.1075 - dense_1306_loss: 0.0919 - dense_1308_loss: 0.1101 - dense_1310_loss: 0.1006 - dense_1312_loss: 0.1413 - dense_1314_loss: 0.1287 - dense_1316_loss: 0.1108 - dense_1318_loss: 0.1417 - dense_1320_loss: 0.1498 - dense_1322_loss: 0.1751 - dense_1324_loss: 0.2185 - dense_1326_loss: 0.3180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30\n",
      " - 41s - loss: 12.1470 - dense_1108_loss: 0.0140 - dense_1110_loss: 0.0144 - dense_1112_loss: 0.0089 - dense_1114_loss: 0.0110 - dense_1116_loss: 0.0131 - dense_1118_loss: 0.0081 - dense_1120_loss: 0.0123 - dense_1122_loss: 0.0164 - dense_1124_loss: 0.0142 - dense_1126_loss: 0.0078 - dense_1128_loss: 0.0120 - dense_1130_loss: 0.0151 - dense_1132_loss: 0.0132 - dense_1134_loss: 0.0136 - dense_1136_loss: 0.0164 - dense_1138_loss: 0.0117 - dense_1140_loss: 0.0108 - dense_1142_loss: 0.0117 - dense_1144_loss: 0.0154 - dense_1146_loss: 0.0146 - dense_1148_loss: 0.0128 - dense_1150_loss: 0.0357 - dense_1152_loss: 0.0217 - dense_1154_loss: 0.0161 - dense_1156_loss: 0.0098 - dense_1158_loss: 0.0206 - dense_1160_loss: 0.0131 - dense_1162_loss: 0.0143 - dense_1164_loss: 0.0136 - dense_1166_loss: 0.0144 - dense_1168_loss: 0.0091 - dense_1170_loss: 0.0134 - dense_1172_loss: 0.0162 - dense_1174_loss: 0.0151 - dense_1176_loss: 0.0275 - dense_1178_loss: 0.0265 - dense_1180_loss: 0.0188 - dense_1182_loss: 0.0242 - dense_1184_loss: 0.0134 - dense_1186_loss: 0.0155 - dense_1188_loss: 0.0199 - dense_1190_loss: 0.0174 - dense_1192_loss: 0.0186 - dense_1194_loss: 0.0106 - dense_1196_loss: 0.0182 - dense_1198_loss: 0.0170 - dense_1200_loss: 0.0156 - dense_1202_loss: 0.0129 - dense_1204_loss: 0.0244 - dense_1206_loss: 0.0189 - dense_1208_loss: 0.0304 - dense_1210_loss: 0.0266 - dense_1212_loss: 0.0249 - dense_1214_loss: 0.0340 - dense_1216_loss: 0.0314 - dense_1218_loss: 0.0186 - dense_1220_loss: 0.0367 - dense_1222_loss: 0.0304 - dense_1224_loss: 0.0178 - dense_1226_loss: 0.0286 - dense_1228_loss: 0.0268 - dense_1230_loss: 0.0390 - dense_1232_loss: 0.0329 - dense_1234_loss: 0.0411 - dense_1236_loss: 0.0265 - dense_1238_loss: 0.0348 - dense_1240_loss: 0.0357 - dense_1242_loss: 0.0291 - dense_1244_loss: 0.0620 - dense_1246_loss: 0.0299 - dense_1248_loss: 0.0261 - dense_1250_loss: 0.0316 - dense_1252_loss: 0.0538 - dense_1254_loss: 0.0502 - dense_1256_loss: 0.0340 - dense_1258_loss: 0.0502 - dense_1260_loss: 0.0342 - dense_1262_loss: 0.0368 - dense_1264_loss: 0.0449 - dense_1266_loss: 0.0387 - dense_1268_loss: 0.0451 - dense_1270_loss: 0.0408 - dense_1272_loss: 0.0481 - dense_1274_loss: 0.0640 - dense_1276_loss: 0.0324 - dense_1278_loss: 0.0706 - dense_1280_loss: 0.0551 - dense_1282_loss: 0.0491 - dense_1284_loss: 0.0648 - dense_1286_loss: 0.0439 - dense_1288_loss: 0.0462 - dense_1290_loss: 0.0643 - dense_1292_loss: 0.0071 - dense_1294_loss: 0.0582 - dense_1296_loss: 0.0535 - dense_1298_loss: 0.0418 - dense_1300_loss: 0.0659 - dense_1302_loss: 0.0823 - dense_1304_loss: 0.0954 - dense_1306_loss: 0.1014 - dense_1308_loss: 0.0794 - dense_1310_loss: 0.0990 - dense_1312_loss: 0.1416 - dense_1314_loss: 0.1167 - dense_1316_loss: 0.1089 - dense_1318_loss: 0.1236 - dense_1320_loss: 0.1492 - dense_1322_loss: 0.1908 - dense_1324_loss: 0.2061 - dense_1326_loss: 0.3262\n",
      "Epoch 23/30\n",
      " - 40s - loss: 12.0334 - dense_1108_loss: 0.0120 - dense_1110_loss: 0.0143 - dense_1112_loss: 0.0076 - dense_1114_loss: 0.0221 - dense_1116_loss: 0.0154 - dense_1118_loss: 0.0057 - dense_1120_loss: 0.0129 - dense_1122_loss: 0.0113 - dense_1124_loss: 0.0137 - dense_1126_loss: 0.0080 - dense_1128_loss: 0.0111 - dense_1130_loss: 0.0144 - dense_1132_loss: 0.0094 - dense_1134_loss: 0.0132 - dense_1136_loss: 0.0242 - dense_1138_loss: 0.0151 - dense_1140_loss: 0.0117 - dense_1142_loss: 0.0131 - dense_1144_loss: 0.0166 - dense_1146_loss: 0.0150 - dense_1148_loss: 0.0142 - dense_1150_loss: 0.0341 - dense_1152_loss: 0.0247 - dense_1154_loss: 0.0137 - dense_1156_loss: 0.0096 - dense_1158_loss: 0.0185 - dense_1160_loss: 0.0139 - dense_1162_loss: 0.0127 - dense_1164_loss: 0.0110 - dense_1166_loss: 0.0151 - dense_1168_loss: 0.0120 - dense_1170_loss: 0.0195 - dense_1172_loss: 0.0168 - dense_1174_loss: 0.0141 - dense_1176_loss: 0.0321 - dense_1178_loss: 0.0239 - dense_1180_loss: 0.0173 - dense_1182_loss: 0.0258 - dense_1184_loss: 0.0169 - dense_1186_loss: 0.0132 - dense_1188_loss: 0.0210 - dense_1190_loss: 0.0242 - dense_1192_loss: 0.0180 - dense_1194_loss: 0.0145 - dense_1196_loss: 0.0194 - dense_1198_loss: 0.0226 - dense_1200_loss: 0.0144 - dense_1202_loss: 0.0142 - dense_1204_loss: 0.0219 - dense_1206_loss: 0.0200 - dense_1208_loss: 0.0303 - dense_1210_loss: 0.0268 - dense_1212_loss: 0.0246 - dense_1214_loss: 0.0277 - dense_1216_loss: 0.0299 - dense_1218_loss: 0.0198 - dense_1220_loss: 0.0377 - dense_1222_loss: 0.0308 - dense_1224_loss: 0.0191 - dense_1226_loss: 0.0256 - dense_1228_loss: 0.0250 - dense_1230_loss: 0.0485 - dense_1232_loss: 0.0378 - dense_1234_loss: 0.0316 - dense_1236_loss: 0.0309 - dense_1238_loss: 0.0316 - dense_1240_loss: 0.0356 - dense_1242_loss: 0.0315 - dense_1244_loss: 0.0433 - dense_1246_loss: 0.0281 - dense_1248_loss: 0.0236 - dense_1250_loss: 0.0415 - dense_1252_loss: 0.0386 - dense_1254_loss: 0.0388 - dense_1256_loss: 0.0299 - dense_1258_loss: 0.0513 - dense_1260_loss: 0.0296 - dense_1262_loss: 0.0401 - dense_1264_loss: 0.0450 - dense_1266_loss: 0.0386 - dense_1268_loss: 0.0434 - dense_1270_loss: 0.0419 - dense_1272_loss: 0.0466 - dense_1274_loss: 0.0698 - dense_1276_loss: 0.0366 - dense_1278_loss: 0.0715 - dense_1280_loss: 0.0395 - dense_1282_loss: 0.0477 - dense_1284_loss: 0.0736 - dense_1286_loss: 0.0414 - dense_1288_loss: 0.0556 - dense_1290_loss: 0.0672 - dense_1292_loss: 0.0071 - dense_1294_loss: 0.0457 - dense_1296_loss: 0.0571 - dense_1298_loss: 0.0399 - dense_1300_loss: 0.0822 - dense_1302_loss: 0.0831 - dense_1304_loss: 0.0977 - dense_1306_loss: 0.0911 - dense_1308_loss: 0.0843 - dense_1310_loss: 0.1053 - dense_1312_loss: 0.1249 - dense_1314_loss: 0.1115 - dense_1316_loss: 0.1160 - dense_1318_loss: 0.1309 - dense_1320_loss: 0.1638 - dense_1322_loss: 0.1550 - dense_1324_loss: 0.2055 - dense_1326_loss: 0.2897\n",
      "Epoch 24/30\n",
      " - 41s - loss: 11.8846 - dense_1108_loss: 0.0110 - dense_1110_loss: 0.0163 - dense_1112_loss: 0.0082 - dense_1114_loss: 0.0180 - dense_1116_loss: 0.0148 - dense_1118_loss: 0.0091 - dense_1120_loss: 0.0125 - dense_1122_loss: 0.0142 - dense_1124_loss: 0.0134 - dense_1126_loss: 0.0088 - dense_1128_loss: 0.0127 - dense_1130_loss: 0.0146 - dense_1132_loss: 0.0144 - dense_1134_loss: 0.0130 - dense_1136_loss: 0.0142 - dense_1138_loss: 0.0125 - dense_1140_loss: 0.0126 - dense_1142_loss: 0.0136 - dense_1144_loss: 0.0103 - dense_1146_loss: 0.0145 - dense_1148_loss: 0.0095 - dense_1150_loss: 0.0218 - dense_1152_loss: 0.0164 - dense_1154_loss: 0.0169 - dense_1156_loss: 0.0106 - dense_1158_loss: 0.0154 - dense_1160_loss: 0.0132 - dense_1162_loss: 0.0180 - dense_1164_loss: 0.0119 - dense_1166_loss: 0.0120 - dense_1168_loss: 0.0103 - dense_1170_loss: 0.0155 - dense_1172_loss: 0.0131 - dense_1174_loss: 0.0113 - dense_1176_loss: 0.0257 - dense_1178_loss: 0.0203 - dense_1180_loss: 0.0275 - dense_1182_loss: 0.0245 - dense_1184_loss: 0.0156 - dense_1186_loss: 0.0153 - dense_1188_loss: 0.0236 - dense_1190_loss: 0.0198 - dense_1192_loss: 0.0169 - dense_1194_loss: 0.0127 - dense_1196_loss: 0.0227 - dense_1198_loss: 0.0192 - dense_1200_loss: 0.0110 - dense_1202_loss: 0.0113 - dense_1204_loss: 0.0235 - dense_1206_loss: 0.0230 - dense_1208_loss: 0.0324 - dense_1210_loss: 0.0285 - dense_1212_loss: 0.0237 - dense_1214_loss: 0.0239 - dense_1216_loss: 0.0282 - dense_1218_loss: 0.0211 - dense_1220_loss: 0.0337 - dense_1222_loss: 0.0268 - dense_1224_loss: 0.0177 - dense_1226_loss: 0.0324 - dense_1228_loss: 0.0244 - dense_1230_loss: 0.0400 - dense_1232_loss: 0.0298 - dense_1234_loss: 0.0289 - dense_1236_loss: 0.0280 - dense_1238_loss: 0.0369 - dense_1240_loss: 0.0349 - dense_1242_loss: 0.0310 - dense_1244_loss: 0.0438 - dense_1246_loss: 0.0271 - dense_1248_loss: 0.0307 - dense_1250_loss: 0.0343 - dense_1252_loss: 0.0461 - dense_1254_loss: 0.0393 - dense_1256_loss: 0.0324 - dense_1258_loss: 0.0410 - dense_1260_loss: 0.0285 - dense_1262_loss: 0.0451 - dense_1264_loss: 0.0386 - dense_1266_loss: 0.0368 - dense_1268_loss: 0.0527 - dense_1270_loss: 0.0399 - dense_1272_loss: 0.0508 - dense_1274_loss: 0.0684 - dense_1276_loss: 0.0389 - dense_1278_loss: 0.0712 - dense_1280_loss: 0.0550 - dense_1282_loss: 0.0548 - dense_1284_loss: 0.0658 - dense_1286_loss: 0.0486 - dense_1288_loss: 0.0562 - dense_1290_loss: 0.0573 - dense_1292_loss: 0.0073 - dense_1294_loss: 0.0485 - dense_1296_loss: 0.0477 - dense_1298_loss: 0.0464 - dense_1300_loss: 0.0595 - dense_1302_loss: 0.0846 - dense_1304_loss: 0.1073 - dense_1306_loss: 0.0952 - dense_1308_loss: 0.0975 - dense_1310_loss: 0.1014 - dense_1312_loss: 0.1465 - dense_1314_loss: 0.1150 - dense_1316_loss: 0.1089 - dense_1318_loss: 0.1355 - dense_1320_loss: 0.1481 - dense_1322_loss: 0.1734 - dense_1324_loss: 0.2128 - dense_1326_loss: 0.2958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30\n",
      " - 41s - loss: 11.8153 - dense_1108_loss: 0.0123 - dense_1110_loss: 0.0162 - dense_1112_loss: 0.0102 - dense_1114_loss: 0.0114 - dense_1116_loss: 0.0157 - dense_1118_loss: 0.0052 - dense_1120_loss: 0.0104 - dense_1122_loss: 0.0179 - dense_1124_loss: 0.0142 - dense_1126_loss: 0.0106 - dense_1128_loss: 0.0133 - dense_1130_loss: 0.0160 - dense_1132_loss: 0.0132 - dense_1134_loss: 0.0137 - dense_1136_loss: 0.0187 - dense_1138_loss: 0.0123 - dense_1140_loss: 0.0161 - dense_1142_loss: 0.0117 - dense_1144_loss: 0.0149 - dense_1146_loss: 0.0191 - dense_1148_loss: 0.0163 - dense_1150_loss: 0.0213 - dense_1152_loss: 0.0343 - dense_1154_loss: 0.0129 - dense_1156_loss: 0.0121 - dense_1158_loss: 0.0164 - dense_1160_loss: 0.0125 - dense_1162_loss: 0.0150 - dense_1164_loss: 0.0080 - dense_1166_loss: 0.0110 - dense_1168_loss: 0.0175 - dense_1170_loss: 0.0165 - dense_1172_loss: 0.0170 - dense_1174_loss: 0.0109 - dense_1176_loss: 0.0228 - dense_1178_loss: 0.0282 - dense_1180_loss: 0.0164 - dense_1182_loss: 0.0253 - dense_1184_loss: 0.0147 - dense_1186_loss: 0.0145 - dense_1188_loss: 0.0199 - dense_1190_loss: 0.0209 - dense_1192_loss: 0.0198 - dense_1194_loss: 0.0117 - dense_1196_loss: 0.0214 - dense_1198_loss: 0.0167 - dense_1200_loss: 0.0203 - dense_1202_loss: 0.0144 - dense_1204_loss: 0.0272 - dense_1206_loss: 0.0192 - dense_1208_loss: 0.0345 - dense_1210_loss: 0.0272 - dense_1212_loss: 0.0248 - dense_1214_loss: 0.0250 - dense_1216_loss: 0.0281 - dense_1218_loss: 0.0190 - dense_1220_loss: 0.0292 - dense_1222_loss: 0.0323 - dense_1224_loss: 0.0191 - dense_1226_loss: 0.0303 - dense_1228_loss: 0.0226 - dense_1230_loss: 0.0417 - dense_1232_loss: 0.0297 - dense_1234_loss: 0.0312 - dense_1236_loss: 0.0220 - dense_1238_loss: 0.0334 - dense_1240_loss: 0.0418 - dense_1242_loss: 0.0276 - dense_1244_loss: 0.0507 - dense_1246_loss: 0.0298 - dense_1248_loss: 0.0315 - dense_1250_loss: 0.0385 - dense_1252_loss: 0.0380 - dense_1254_loss: 0.0335 - dense_1256_loss: 0.0319 - dense_1258_loss: 0.0445 - dense_1260_loss: 0.0299 - dense_1262_loss: 0.0447 - dense_1264_loss: 0.0374 - dense_1266_loss: 0.0372 - dense_1268_loss: 0.0424 - dense_1270_loss: 0.0455 - dense_1272_loss: 0.0475 - dense_1274_loss: 0.0660 - dense_1276_loss: 0.0432 - dense_1278_loss: 0.0756 - dense_1280_loss: 0.0419 - dense_1282_loss: 0.0462 - dense_1284_loss: 0.0568 - dense_1286_loss: 0.0447 - dense_1288_loss: 0.0558 - dense_1290_loss: 0.0665 - dense_1292_loss: 0.0073 - dense_1294_loss: 0.0514 - dense_1296_loss: 0.0504 - dense_1298_loss: 0.0387 - dense_1300_loss: 0.0580 - dense_1302_loss: 0.0844 - dense_1304_loss: 0.1034 - dense_1306_loss: 0.0965 - dense_1308_loss: 0.1081 - dense_1310_loss: 0.1109 - dense_1312_loss: 0.1367 - dense_1314_loss: 0.1154 - dense_1316_loss: 0.1372 - dense_1318_loss: 0.1312 - dense_1320_loss: 0.1398 - dense_1322_loss: 0.1607 - dense_1324_loss: 0.1999 - dense_1326_loss: 0.3006\n",
      "Epoch 26/30\n",
      " - 39s - loss: 11.6866 - dense_1108_loss: 0.0098 - dense_1110_loss: 0.0092 - dense_1112_loss: 0.0087 - dense_1114_loss: 0.0098 - dense_1116_loss: 0.0123 - dense_1118_loss: 0.0061 - dense_1120_loss: 0.0118 - dense_1122_loss: 0.0228 - dense_1124_loss: 0.0134 - dense_1126_loss: 0.0063 - dense_1128_loss: 0.0129 - dense_1130_loss: 0.0177 - dense_1132_loss: 0.0123 - dense_1134_loss: 0.0153 - dense_1136_loss: 0.0150 - dense_1138_loss: 0.0132 - dense_1140_loss: 0.0184 - dense_1142_loss: 0.0105 - dense_1144_loss: 0.0127 - dense_1146_loss: 0.0165 - dense_1148_loss: 0.0102 - dense_1150_loss: 0.0163 - dense_1152_loss: 0.0202 - dense_1154_loss: 0.0139 - dense_1156_loss: 0.0137 - dense_1158_loss: 0.0193 - dense_1160_loss: 0.0145 - dense_1162_loss: 0.0126 - dense_1164_loss: 0.0102 - dense_1166_loss: 0.0142 - dense_1168_loss: 0.0098 - dense_1170_loss: 0.0142 - dense_1172_loss: 0.0132 - dense_1174_loss: 0.0124 - dense_1176_loss: 0.0209 - dense_1178_loss: 0.0262 - dense_1180_loss: 0.0237 - dense_1182_loss: 0.0207 - dense_1184_loss: 0.0123 - dense_1186_loss: 0.0175 - dense_1188_loss: 0.0226 - dense_1190_loss: 0.0194 - dense_1192_loss: 0.0168 - dense_1194_loss: 0.0090 - dense_1196_loss: 0.0214 - dense_1198_loss: 0.0164 - dense_1200_loss: 0.0154 - dense_1202_loss: 0.0159 - dense_1204_loss: 0.0223 - dense_1206_loss: 0.0195 - dense_1208_loss: 0.0313 - dense_1210_loss: 0.0302 - dense_1212_loss: 0.0250 - dense_1214_loss: 0.0228 - dense_1216_loss: 0.0288 - dense_1218_loss: 0.0245 - dense_1220_loss: 0.0294 - dense_1222_loss: 0.0350 - dense_1224_loss: 0.0263 - dense_1226_loss: 0.0264 - dense_1228_loss: 0.0217 - dense_1230_loss: 0.0481 - dense_1232_loss: 0.0303 - dense_1234_loss: 0.0292 - dense_1236_loss: 0.0259 - dense_1238_loss: 0.0320 - dense_1240_loss: 0.0366 - dense_1242_loss: 0.0280 - dense_1244_loss: 0.0475 - dense_1246_loss: 0.0289 - dense_1248_loss: 0.0250 - dense_1250_loss: 0.0353 - dense_1252_loss: 0.0438 - dense_1254_loss: 0.0476 - dense_1256_loss: 0.0337 - dense_1258_loss: 0.0465 - dense_1260_loss: 0.0293 - dense_1262_loss: 0.0409 - dense_1264_loss: 0.0441 - dense_1266_loss: 0.0377 - dense_1268_loss: 0.0432 - dense_1270_loss: 0.0426 - dense_1272_loss: 0.0485 - dense_1274_loss: 0.0584 - dense_1276_loss: 0.0338 - dense_1278_loss: 0.0678 - dense_1280_loss: 0.0415 - dense_1282_loss: 0.0507 - dense_1284_loss: 0.0592 - dense_1286_loss: 0.0487 - dense_1288_loss: 0.0462 - dense_1290_loss: 0.0669 - dense_1292_loss: 0.0070 - dense_1294_loss: 0.0535 - dense_1296_loss: 0.0512 - dense_1298_loss: 0.0456 - dense_1300_loss: 0.0624 - dense_1302_loss: 0.0870 - dense_1304_loss: 0.1065 - dense_1306_loss: 0.0913 - dense_1308_loss: 0.1096 - dense_1310_loss: 0.1087 - dense_1312_loss: 0.1371 - dense_1314_loss: 0.1078 - dense_1316_loss: 0.1140 - dense_1318_loss: 0.1330 - dense_1320_loss: 0.1518 - dense_1322_loss: 0.1765 - dense_1324_loss: 0.2132 - dense_1326_loss: 0.2922\n",
      "Epoch 27/30\n",
      " - 40s - loss: 11.5938 - dense_1108_loss: 0.0099 - dense_1110_loss: 0.0125 - dense_1112_loss: 0.0079 - dense_1114_loss: 0.0107 - dense_1116_loss: 0.0131 - dense_1118_loss: 0.0060 - dense_1120_loss: 0.0108 - dense_1122_loss: 0.0157 - dense_1124_loss: 0.0133 - dense_1126_loss: 0.0085 - dense_1128_loss: 0.0101 - dense_1130_loss: 0.0148 - dense_1132_loss: 0.0131 - dense_1134_loss: 0.0144 - dense_1136_loss: 0.0169 - dense_1138_loss: 0.0108 - dense_1140_loss: 0.0108 - dense_1142_loss: 0.0111 - dense_1144_loss: 0.0128 - dense_1146_loss: 0.0133 - dense_1148_loss: 0.0121 - dense_1150_loss: 0.0198 - dense_1152_loss: 0.0207 - dense_1154_loss: 0.0200 - dense_1156_loss: 0.0126 - dense_1158_loss: 0.0179 - dense_1160_loss: 0.0123 - dense_1162_loss: 0.0130 - dense_1164_loss: 0.0089 - dense_1166_loss: 0.0127 - dense_1168_loss: 0.0097 - dense_1170_loss: 0.0151 - dense_1172_loss: 0.0167 - dense_1174_loss: 0.0148 - dense_1176_loss: 0.0198 - dense_1178_loss: 0.0312 - dense_1180_loss: 0.0135 - dense_1182_loss: 0.0192 - dense_1184_loss: 0.0141 - dense_1186_loss: 0.0155 - dense_1188_loss: 0.0246 - dense_1190_loss: 0.0239 - dense_1192_loss: 0.0179 - dense_1194_loss: 0.0094 - dense_1196_loss: 0.0187 - dense_1198_loss: 0.0177 - dense_1200_loss: 0.0139 - dense_1202_loss: 0.0181 - dense_1204_loss: 0.0214 - dense_1206_loss: 0.0158 - dense_1208_loss: 0.0315 - dense_1210_loss: 0.0256 - dense_1212_loss: 0.0230 - dense_1214_loss: 0.0269 - dense_1216_loss: 0.0277 - dense_1218_loss: 0.0232 - dense_1220_loss: 0.0299 - dense_1222_loss: 0.0322 - dense_1224_loss: 0.0201 - dense_1226_loss: 0.0426 - dense_1228_loss: 0.0217 - dense_1230_loss: 0.0483 - dense_1232_loss: 0.0413 - dense_1234_loss: 0.0316 - dense_1236_loss: 0.0259 - dense_1238_loss: 0.0368 - dense_1240_loss: 0.0366 - dense_1242_loss: 0.0337 - dense_1244_loss: 0.0436 - dense_1246_loss: 0.0283 - dense_1248_loss: 0.0278 - dense_1250_loss: 0.0377 - dense_1252_loss: 0.0451 - dense_1254_loss: 0.0442 - dense_1256_loss: 0.0301 - dense_1258_loss: 0.0588 - dense_1260_loss: 0.0408 - dense_1262_loss: 0.0408 - dense_1264_loss: 0.0448 - dense_1266_loss: 0.0355 - dense_1268_loss: 0.0482 - dense_1270_loss: 0.0431 - dense_1272_loss: 0.0451 - dense_1274_loss: 0.0695 - dense_1276_loss: 0.0357 - dense_1278_loss: 0.0701 - dense_1280_loss: 0.0379 - dense_1282_loss: 0.0502 - dense_1284_loss: 0.0600 - dense_1286_loss: 0.0475 - dense_1288_loss: 0.0469 - dense_1290_loss: 0.0684 - dense_1292_loss: 0.0074 - dense_1294_loss: 0.0490 - dense_1296_loss: 0.0578 - dense_1298_loss: 0.0450 - dense_1300_loss: 0.0794 - dense_1302_loss: 0.0844 - dense_1304_loss: 0.0978 - dense_1306_loss: 0.1082 - dense_1308_loss: 0.0916 - dense_1310_loss: 0.0966 - dense_1312_loss: 0.1586 - dense_1314_loss: 0.1116 - dense_1316_loss: 0.1002 - dense_1318_loss: 0.1340 - dense_1320_loss: 0.1531 - dense_1322_loss: 0.1645 - dense_1324_loss: 0.1963 - dense_1326_loss: 0.2835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30\n",
      " - 41s - loss: 11.4433 - dense_1108_loss: 0.0111 - dense_1110_loss: 0.0101 - dense_1112_loss: 0.0087 - dense_1114_loss: 0.0091 - dense_1116_loss: 0.0085 - dense_1118_loss: 0.0058 - dense_1120_loss: 0.0100 - dense_1122_loss: 0.0145 - dense_1124_loss: 0.0112 - dense_1126_loss: 0.0090 - dense_1128_loss: 0.0134 - dense_1130_loss: 0.0150 - dense_1132_loss: 0.0151 - dense_1134_loss: 0.0183 - dense_1136_loss: 0.0245 - dense_1138_loss: 0.0124 - dense_1140_loss: 0.0148 - dense_1142_loss: 0.0163 - dense_1144_loss: 0.0116 - dense_1146_loss: 0.0151 - dense_1148_loss: 0.0139 - dense_1150_loss: 0.0188 - dense_1152_loss: 0.0142 - dense_1154_loss: 0.0118 - dense_1156_loss: 0.0159 - dense_1158_loss: 0.0210 - dense_1160_loss: 0.0128 - dense_1162_loss: 0.0111 - dense_1164_loss: 0.0095 - dense_1166_loss: 0.0104 - dense_1168_loss: 0.0125 - dense_1170_loss: 0.0139 - dense_1172_loss: 0.0120 - dense_1174_loss: 0.0104 - dense_1176_loss: 0.0276 - dense_1178_loss: 0.0244 - dense_1180_loss: 0.0181 - dense_1182_loss: 0.0209 - dense_1184_loss: 0.0148 - dense_1186_loss: 0.0154 - dense_1188_loss: 0.0203 - dense_1190_loss: 0.0300 - dense_1192_loss: 0.0222 - dense_1194_loss: 0.0116 - dense_1196_loss: 0.0215 - dense_1198_loss: 0.0189 - dense_1200_loss: 0.0128 - dense_1202_loss: 0.0150 - dense_1204_loss: 0.0205 - dense_1206_loss: 0.0257 - dense_1208_loss: 0.0280 - dense_1210_loss: 0.0326 - dense_1212_loss: 0.0225 - dense_1214_loss: 0.0277 - dense_1216_loss: 0.0238 - dense_1218_loss: 0.0199 - dense_1220_loss: 0.0355 - dense_1222_loss: 0.0341 - dense_1224_loss: 0.0170 - dense_1226_loss: 0.0314 - dense_1228_loss: 0.0225 - dense_1230_loss: 0.0512 - dense_1232_loss: 0.0361 - dense_1234_loss: 0.0281 - dense_1236_loss: 0.0301 - dense_1238_loss: 0.0343 - dense_1240_loss: 0.0327 - dense_1242_loss: 0.0332 - dense_1244_loss: 0.0445 - dense_1246_loss: 0.0282 - dense_1248_loss: 0.0284 - dense_1250_loss: 0.0393 - dense_1252_loss: 0.0446 - dense_1254_loss: 0.0508 - dense_1256_loss: 0.0351 - dense_1258_loss: 0.0444 - dense_1260_loss: 0.0316 - dense_1262_loss: 0.0400 - dense_1264_loss: 0.0380 - dense_1266_loss: 0.0372 - dense_1268_loss: 0.0399 - dense_1270_loss: 0.0392 - dense_1272_loss: 0.0455 - dense_1274_loss: 0.0593 - dense_1276_loss: 0.0364 - dense_1278_loss: 0.0666 - dense_1280_loss: 0.0432 - dense_1282_loss: 0.0499 - dense_1284_loss: 0.0695 - dense_1286_loss: 0.0482 - dense_1288_loss: 0.0451 - dense_1290_loss: 0.0668 - dense_1292_loss: 0.0067 - dense_1294_loss: 0.0472 - dense_1296_loss: 0.0615 - dense_1298_loss: 0.0445 - dense_1300_loss: 0.0595 - dense_1302_loss: 0.0806 - dense_1304_loss: 0.0976 - dense_1306_loss: 0.0919 - dense_1308_loss: 0.0863 - dense_1310_loss: 0.0972 - dense_1312_loss: 0.1299 - dense_1314_loss: 0.1084 - dense_1316_loss: 0.1020 - dense_1318_loss: 0.1311 - dense_1320_loss: 0.1530 - dense_1322_loss: 0.1772 - dense_1324_loss: 0.1920 - dense_1326_loss: 0.3029\n",
      "Epoch 29/30\n",
      " - 39s - loss: 11.4344 - dense_1108_loss: 0.0125 - dense_1110_loss: 0.0123 - dense_1112_loss: 0.0077 - dense_1114_loss: 0.0120 - dense_1116_loss: 0.0169 - dense_1118_loss: 0.0042 - dense_1120_loss: 0.0097 - dense_1122_loss: 0.0134 - dense_1124_loss: 0.0152 - dense_1126_loss: 0.0077 - dense_1128_loss: 0.0101 - dense_1130_loss: 0.0145 - dense_1132_loss: 0.0094 - dense_1134_loss: 0.0164 - dense_1136_loss: 0.0158 - dense_1138_loss: 0.0145 - dense_1140_loss: 0.0135 - dense_1142_loss: 0.0206 - dense_1144_loss: 0.0131 - dense_1146_loss: 0.0143 - dense_1148_loss: 0.0130 - dense_1150_loss: 0.0195 - dense_1152_loss: 0.0215 - dense_1154_loss: 0.0152 - dense_1156_loss: 0.0127 - dense_1158_loss: 0.0156 - dense_1160_loss: 0.0120 - dense_1162_loss: 0.0151 - dense_1164_loss: 0.0094 - dense_1166_loss: 0.0152 - dense_1168_loss: 0.0096 - dense_1170_loss: 0.0138 - dense_1172_loss: 0.0181 - dense_1174_loss: 0.0148 - dense_1176_loss: 0.0215 - dense_1178_loss: 0.0205 - dense_1180_loss: 0.0136 - dense_1182_loss: 0.0253 - dense_1184_loss: 0.0145 - dense_1186_loss: 0.0147 - dense_1188_loss: 0.0195 - dense_1190_loss: 0.0256 - dense_1192_loss: 0.0187 - dense_1194_loss: 0.0121 - dense_1196_loss: 0.0197 - dense_1198_loss: 0.0263 - dense_1200_loss: 0.0130 - dense_1202_loss: 0.0104 - dense_1204_loss: 0.0193 - dense_1206_loss: 0.0232 - dense_1208_loss: 0.0389 - dense_1210_loss: 0.0247 - dense_1212_loss: 0.0229 - dense_1214_loss: 0.0293 - dense_1216_loss: 0.0243 - dense_1218_loss: 0.0210 - dense_1220_loss: 0.0356 - dense_1222_loss: 0.0308 - dense_1224_loss: 0.0180 - dense_1226_loss: 0.0381 - dense_1228_loss: 0.0270 - dense_1230_loss: 0.0586 - dense_1232_loss: 0.0368 - dense_1234_loss: 0.0321 - dense_1236_loss: 0.0255 - dense_1238_loss: 0.0334 - dense_1240_loss: 0.0330 - dense_1242_loss: 0.0301 - dense_1244_loss: 0.0462 - dense_1246_loss: 0.0254 - dense_1248_loss: 0.0343 - dense_1250_loss: 0.0356 - dense_1252_loss: 0.0391 - dense_1254_loss: 0.0391 - dense_1256_loss: 0.0322 - dense_1258_loss: 0.0407 - dense_1260_loss: 0.0324 - dense_1262_loss: 0.0413 - dense_1264_loss: 0.0361 - dense_1266_loss: 0.0388 - dense_1268_loss: 0.0480 - dense_1270_loss: 0.0451 - dense_1272_loss: 0.0501 - dense_1274_loss: 0.0622 - dense_1276_loss: 0.0410 - dense_1278_loss: 0.0662 - dense_1280_loss: 0.0359 - dense_1282_loss: 0.0485 - dense_1284_loss: 0.0655 - dense_1286_loss: 0.0590 - dense_1288_loss: 0.0467 - dense_1290_loss: 0.0727 - dense_1292_loss: 0.0073 - dense_1294_loss: 0.0475 - dense_1296_loss: 0.0535 - dense_1298_loss: 0.0489 - dense_1300_loss: 0.0620 - dense_1302_loss: 0.0735 - dense_1304_loss: 0.1143 - dense_1306_loss: 0.0965 - dense_1308_loss: 0.0897 - dense_1310_loss: 0.0948 - dense_1312_loss: 0.1242 - dense_1314_loss: 0.1214 - dense_1316_loss: 0.1135 - dense_1318_loss: 0.1215 - dense_1320_loss: 0.1407 - dense_1322_loss: 0.1603 - dense_1324_loss: 0.2142 - dense_1326_loss: 0.3192\n",
      "Epoch 30/30\n",
      " - 37s - loss: 11.4074 - dense_1108_loss: 0.0118 - dense_1110_loss: 0.0140 - dense_1112_loss: 0.0076 - dense_1114_loss: 0.0113 - dense_1116_loss: 0.0140 - dense_1118_loss: 0.0064 - dense_1120_loss: 0.0115 - dense_1122_loss: 0.0184 - dense_1124_loss: 0.0127 - dense_1126_loss: 0.0083 - dense_1128_loss: 0.0134 - dense_1130_loss: 0.0147 - dense_1132_loss: 0.0088 - dense_1134_loss: 0.0153 - dense_1136_loss: 0.0154 - dense_1138_loss: 0.0157 - dense_1140_loss: 0.0149 - dense_1142_loss: 0.0090 - dense_1144_loss: 0.0164 - dense_1146_loss: 0.0130 - dense_1148_loss: 0.0122 - dense_1150_loss: 0.0229 - dense_1152_loss: 0.0220 - dense_1154_loss: 0.0132 - dense_1156_loss: 0.0130 - dense_1158_loss: 0.0212 - dense_1160_loss: 0.0146 - dense_1162_loss: 0.0213 - dense_1164_loss: 0.0113 - dense_1166_loss: 0.0133 - dense_1168_loss: 0.0084 - dense_1170_loss: 0.0146 - dense_1172_loss: 0.0205 - dense_1174_loss: 0.0122 - dense_1176_loss: 0.0181 - dense_1178_loss: 0.0214 - dense_1180_loss: 0.0215 - dense_1182_loss: 0.0231 - dense_1184_loss: 0.0142 - dense_1186_loss: 0.0138 - dense_1188_loss: 0.0207 - dense_1190_loss: 0.0204 - dense_1192_loss: 0.0229 - dense_1194_loss: 0.0100 - dense_1196_loss: 0.0204 - dense_1198_loss: 0.0206 - dense_1200_loss: 0.0201 - dense_1202_loss: 0.0134 - dense_1204_loss: 0.0221 - dense_1206_loss: 0.0155 - dense_1208_loss: 0.0300 - dense_1210_loss: 0.0257 - dense_1212_loss: 0.0298 - dense_1214_loss: 0.0228 - dense_1216_loss: 0.0265 - dense_1218_loss: 0.0201 - dense_1220_loss: 0.0336 - dense_1222_loss: 0.0316 - dense_1224_loss: 0.0158 - dense_1226_loss: 0.0295 - dense_1228_loss: 0.0219 - dense_1230_loss: 0.0405 - dense_1232_loss: 0.0350 - dense_1234_loss: 0.0357 - dense_1236_loss: 0.0241 - dense_1238_loss: 0.0319 - dense_1240_loss: 0.0399 - dense_1242_loss: 0.0344 - dense_1244_loss: 0.0581 - dense_1246_loss: 0.0343 - dense_1248_loss: 0.0285 - dense_1250_loss: 0.0340 - dense_1252_loss: 0.0524 - dense_1254_loss: 0.0425 - dense_1256_loss: 0.0333 - dense_1258_loss: 0.0491 - dense_1260_loss: 0.0281 - dense_1262_loss: 0.0385 - dense_1264_loss: 0.0395 - dense_1266_loss: 0.0348 - dense_1268_loss: 0.0427 - dense_1270_loss: 0.0476 - dense_1272_loss: 0.0518 - dense_1274_loss: 0.0567 - dense_1276_loss: 0.0355 - dense_1278_loss: 0.0688 - dense_1280_loss: 0.0421 - dense_1282_loss: 0.0479 - dense_1284_loss: 0.0559 - dense_1286_loss: 0.0435 - dense_1288_loss: 0.0477 - dense_1290_loss: 0.0780 - dense_1292_loss: 0.0068 - dense_1294_loss: 0.0449 - dense_1296_loss: 0.0825 - dense_1298_loss: 0.0434 - dense_1300_loss: 0.0649 - dense_1302_loss: 0.0781 - dense_1304_loss: 0.0959 - dense_1306_loss: 0.1047 - dense_1308_loss: 0.1056 - dense_1310_loss: 0.1181 - dense_1312_loss: 0.1287 - dense_1314_loss: 0.1172 - dense_1316_loss: 0.1183 - dense_1318_loss: 0.1356 - dense_1320_loss: 0.1498 - dense_1322_loss: 0.1556 - dense_1324_loss: 0.2020 - dense_1326_loss: 0.3087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration for fitting =  1159.0961771011353\n",
      "More than  11000  predictions have been parsed.  \n",
      "Performance for MTL-D NN = 0.110222\n"
     ]
    }
   ],
   "source": [
    "# we assume that CV for model selection has already been performed!\n",
    "MTLD = MTL_Drop(2000,20,0.2,0.001)\n",
    "t0=time()\n",
    "MTLD.fit( np.array([Fingerprints[x] for x in Compounds]), [x for x in DTI.T], epochs=30, batch_size=128, verbose=0, use_multiprocessing=True )\n",
    "print(\"Duration for fitting = \", time()-t0)\n",
    "temp = Evaluate( Interactions_valid, Compounds, MTLD, Fingerprints)\n",
    "\n",
    "theta=0.10\n",
    "\n",
    "train_size = len(Interactions_train)\n",
    "count = 1 # just a trigger for the next loop\n",
    "while (train_size<0.5e6) & (count>0):\n",
    "# we need to stop after we have no new predictions or we have enough (10K+)\n",
    "    count=0\n",
    "    for x_new in Compounds:\n",
    "        preds, H = mulpredict(MTLD, np.array( Fingerprints[x_new]).reshape(1,-1), 110, 20, True)\n",
    "        # impute accordingly\n",
    "        for t in range(110):\n",
    "            if (H[t]< theta) & (DTI[Labels_Comp[x_new],t] == 10):\n",
    "                DTI[Labels_Comp[x_new],t] = preds[t] # update the train set\n",
    "                count+=1\n",
    "        if list(Compounds).index(x_new) % 100 == 0:\n",
    "            print(f\"\\rMore than\", list(Compounds).index(x_new) ,\"compounds have been parsed with \",count,\"new values.\", end =\" \")\n",
    "    print(count,\" new values where imputed.\")\n",
    "    train_size += count\n",
    "    \n",
    "    if count >0 :\n",
    "        t0 = time()\n",
    "        MTLD.fit( np.array([Fingerprints[x] for x in Compounds]), [x for x in DTI.T], epochs=30, batch_size=128, verbose=0, use_multiprocessing=True )\n",
    "        print(\"Duration for fitting = \", time()-t0)\n",
    "        temp = Evaluate( Interactions_valid, Compounds, MTLD, Fingerprints)\n",
    "    # repeat if enough points were predicted confidently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem, t\n",
    "\n",
    "def Imputer(model, NewID, Fingerprints, threshold=0.2):\n",
    "    \n",
    "    X_new = []\n",
    "    for cid in NewID:\n",
    "        x_test = np.array( Fingerprints[cid] ).reshape(1,-1)\n",
    "        preds=[]\n",
    "        for DTR in model.estimators_:\n",
    "            preds.append(DTR.predict(x_test) )\n",
    "#         if np.std(preds)<=threshold:\n",
    "#             X_new.append( [ cid, np.mean(preds)] )\n",
    "        std_err = sem(preds)\n",
    "        h = std_err * t.ppf((1 + 0.95) / 2, len(preds) - 1)\n",
    "        if 2*h<=threshold:\n",
    "            X_new.append( [ cid, np.mean(preds)] )\n",
    "    if len(X_new)>0:\n",
    "        print(\"{0} new values were imputed!\".format(len(X_new)))\n",
    "    else:\n",
    "        print(\"No confident values were found.\")\n",
    "    return X_new\n",
    "\n",
    "def Evaluate( TARGET, MODEL, validationset, prnt=False ):\n",
    "    True_temp = []; Pred_temp = []\n",
    "    with open( validationset, 'r') as file:\n",
    "        # no header on this file\n",
    "        for line in file:\n",
    "            tokens = line.split()\n",
    "            if tokens[0]==TARGET:\n",
    "                True_temp.append( float(tokens[2]) )\n",
    "                x_test = np.array( Fingerprints[tokens[1]] ).reshape(1,-1)\n",
    "                Pred_temp.append( MODEL.predict( x_test ) )\n",
    "    r2 = r2_score(True_temp,Pred_temp)\n",
    "    if prnt:\n",
    "        print(\"R2-score after {0} points = {1:.4f} \".format(len(True_temp), r2 ) )\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation without imputation = 0.4265 \n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.4265 \n",
      "More than 0 targets are processed\n",
      "Evaluation without imputation = 0.3326 \n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.3326 \n",
      "Evaluation without imputation = 0.4879 \n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.4879 \n",
      "Evaluation without imputation = 0.6611 \n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.6611 \n",
      "Evaluation without imputation = 0.0597 \n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.0597 \n",
      "Evaluation without imputation = 0.9904 \n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.9904 \n",
      "Evaluation without imputation = 0.3546 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-e3af4299ce78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# update the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mNewIDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCompounds\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTrain_CIDs\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# terra incognito\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mX_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImputer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNewIDs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFingerprints\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX_new\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-e1efaef6c3fd>\u001b[0m in \u001b[0;36mImputer\u001b[0;34m(model, NewID, Fingerprints, threshold)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mpreds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mDTR\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDTR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#         if np.std(preds)<=threshold:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#             X_new.append( [ cid, np.mean(preds)] )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tree_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Target_info = {} # this is a \"global\" variable\n",
    "theta=0.1\n",
    "count=0\n",
    "# param_grid={ 'max_depth':[4,5,7,10,12], 'n_estimators':[25,50,100,150],} # 'max_features':['sqrt','auto']\n",
    "for target in Targets:\n",
    "    Target_info[target] = {}\n",
    "#         \n",
    "\n",
    "    # define the train set\n",
    "    X_train=[]; Y_train=[]\n",
    "    Train_CIDs = []\n",
    "    for point in Interactions_train:\n",
    "        if point[0]==target:\n",
    "            X_train.append( Fingerprints[point[1]] )\n",
    "            Y_train.append( float(point[2]) )\n",
    "            Train_CIDs.append( point[1] )\n",
    "    Target_info[target]['train_size']=len(Y_train) # add info\n",
    "    \n",
    "    with open( 'ECFP/TrainedModels/RF_'+target+'_'+'pIC50new.sav', 'rb') as f:\n",
    "            MODEL = pickle.load( f )\n",
    "            \n",
    "#     if os.path.isfile('TrainedModels/RF_'+target+'_'+'pIC50before.sav'):\n",
    "#         # model is already trained - load:\n",
    "#         with open( 'TrainedModels/RF_'+target+'_'+'pIC50before.sav', 'rb') as f:\n",
    "#             MODEL = pickle.load( f )\n",
    "#     else:\n",
    "#         print(\"Selecting parameters with CV\")\n",
    "#         cvr = GridSearchCV(RandomForestRegressor(random_state=2019), param_grid, cv=nfolds, n_jobs=njobs, iid=True)\n",
    "#         # here comes the long step\n",
    "#         cvr.fit(X_train, Y_train)\n",
    "#         # select best parametrisation and train to the complete train-set\n",
    "#         MODEL = RandomForestRegressor( n_estimators= cvr.best_params_['n_estimators'],max_depth=cvr.best_params_['max_depth'], random_state=2019)\n",
    "#         # now save:\n",
    "#         pickle.dump(MODEL, open('TrainedModels/RF_'+target+'_'+'pIC50before.sav', 'wb'))\n",
    "\n",
    "    # MODEL = KerasRegressor(build_fn=mymodel, init=-4.5, lamda=0.2, epochs=250, batch_size=20, verbose=0)\n",
    "    # X_train = np.array( X_train )\n",
    "    MODEL.fit(X_train,Y_train)\n",
    "    Target_info[target]['RF_train_r2'] = MODEL.score( X_train,  Y_train) # add info\n",
    "    \n",
    "    True_temp = []\n",
    "    Pred_temp = []\n",
    "    for point in Interactions_valid:\n",
    "        if point[0]==target:\n",
    "            True_temp.append( float(point[2]) )\n",
    "            x_test = np.array( Fingerprints[point[1]] ).reshape(1,-1)\n",
    "            Pred_temp.append( MODEL.predict( x_test ) )\n",
    "    Target_info[target][\"first_r2\"] = r2_score(True_temp,Pred_temp)\n",
    "    print(\"Evaluation without imputation = %.4f \" % Target_info[target][\"first_r2\"] )\n",
    "    \n",
    "#     print(\"Imputing confident values...\")\n",
    "    X_new = [1] # just a trigger for the next loop\n",
    "    while  (len(X_new)>0) & (len(Train_CIDs)<2000):\n",
    "    # we need to stop after we have no new predictions or we have enough (10K+)\n",
    "        # update the train set\n",
    "        NewIDs = [x for x in Compounds if x not in Train_CIDs] # terra incognito\n",
    "        X_new = Imputer(MODEL, NewIDs, Fingerprints, threshold=theta)\n",
    "        \n",
    "        for point in X_new:\n",
    "            X_train.append( Fingerprints[point[0]] )\n",
    "            Y_train.append( float(point[1]) )\n",
    "            Train_CIDs.append( point[0] )\n",
    "        # re-train\n",
    "        MODEL.fit(np.array( X_train ),Y_train)\n",
    "        \n",
    "    # RE-SELECT parameters\n",
    "#     cvr = GridSearchCV(RandomForestRegressor(random_state=2019), param_grid, cv=nfolds, n_jobs=njobs, iid=True)\n",
    "#     cvr.fit(X_train, Y_train)\n",
    "    # select best parametrisation and train to the complete train-set\n",
    "#     MODEL = RandomForestRegressor( n_estimators= cvr.best_params_['n_estimators'], max_depth=cvr.best_params_['max_depth'], random_state=2019)\n",
    "#     MODEL.fit(np.array( X_train ),Y_train)\n",
    "    Target_info[target][\"model\"] = MODEL\n",
    "    # evaluate again as before \n",
    "    True_temp = []\n",
    "    Pred_temp = []\n",
    "    for point in Interactions_valid:\n",
    "        if point[0]==target:\n",
    "            True_temp.append( float(point[2]) )\n",
    "            x_test = np.array( Fingerprints[point[1]] ).reshape(1,-1)\n",
    "            Pred_temp.append( MODEL.predict( x_test ) )\n",
    "    Target_info[target][\"after_r2\"] = r2_score(True_temp,Pred_temp)\n",
    "    print(\"Re-evaluate after imputation: %.4f \" % Target_info[target][\"after_r2\"] )\n",
    "    \n",
    "    if count%25==0:\n",
    "        print(\"More than %d targets are processed\" % count)\n",
    "#         print(\"Mean score so far: %f\" % np.mean(Scores_RF_train))\n",
    "    count+=1\n",
    "    \n",
    "# print(\"Mean score for RF during training = %f\" % np.mean(Scores_RF_train) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Target_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ec4623dd0172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mT1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTarget_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first_r2'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mT2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTarget_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'after_r2'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-ec4623dd0172>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mT1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTarget_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'first_r2'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mT2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTarget_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'after_r2'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mTargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Target_info' is not defined"
     ]
    }
   ],
   "source": [
    "T1 = [Target_info[t]['first_r2']  for t in Targets[40:]]\n",
    "T2 = [Target_info[t]['after_r2']  for t in Targets[40:]]\n",
    "\n",
    "plt.plot(T1,T2,'.')\n",
    "plt.plot([-0.2,1],[-0.2,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
