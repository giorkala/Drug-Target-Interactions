{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time benchmarking for every method\n",
    "Reproduce results, compare algorithms wrt accuracy and training. **No need to train anything!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/homes/kalantzi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/homes/kalantzi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/homes/kalantzi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/homes/kalantzi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/homes/kalantzi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/homes/kalantzi/.local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import keras, pickle \n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # this is for making a model like every other in scikit\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "np.random.seed(42)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(42)\n",
    "\n",
    "TRAINEDMODELS='ECFP/TrainedModels/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 110 targets and 23167 compounds currently loaded with 56392 interactions.\n",
      "A DTI matrix would be 2.213% dense!\n",
      "New data: 4.395417284295644 | 2.588704763200101\n",
      "23167 fingerprints were loaded!\n",
      "The sizes for train and validation sets are 45114 and 11278 respectivelly\n"
     ]
    }
   ],
   "source": [
    "Interactions_train = []    \n",
    "with open(\"Interactions_Trainset.tab\",'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        # 'Target-ID', 'Compound-ID', 'pIC50'  \n",
    "        Interactions_train.append( [tokens[0], tokens[1], float(tokens[2]) ])\n",
    "\n",
    "Interactions_valid = []        \n",
    "with open(\"Interactions_Validset.tab\",'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        # 'Target-ID', 'Compound-ID', 'pIC50'  \n",
    "        Interactions_valid.append( [tokens[0], tokens[1], float(tokens[2]) ])\n",
    "\n",
    "Interactions = [x for x in Interactions_train]\n",
    "Interactions.extend(Interactions_valid)\n",
    "# we use a dataframe to quickly sort targets wrt #compounds:\n",
    "DF = pd.DataFrame( Interactions, columns =['Target-ID', 'Compound-ID','Std-value']) \n",
    "temp = DF.groupby(['Target-ID']).agg('count').sort_values(by='Compound-ID') # count the number of molecules\n",
    "Targets = list(temp.index)\n",
    "Compounds = np.unique(DF['Compound-ID'])\n",
    "del temp, DF\n",
    "\n",
    "nT=len(Targets); nC=len(Compounds)\n",
    "\n",
    "print(\"There are {0} targets and {1} compounds currently loaded with {2} interactions.\".format(nT,nC,len(Interactions)))\n",
    "print(\"A DTI matrix would be {0:.4}% dense!\".format(100.0*len(Interactions)/nT/nC ))\n",
    "\n",
    "Labels_Targ = dict()\n",
    "indx=0\n",
    "for x in Targets:\n",
    "    Labels_Targ[x]=indx\n",
    "    indx+=1\n",
    "    \n",
    "Labels_Comp = dict()\n",
    "indx=0\n",
    "for x in Compounds:\n",
    "    Labels_Comp[x]=indx\n",
    "    indx+=1\n",
    "\n",
    "# Initialize sparse matrix - this will be binary\n",
    "DTI = 10*np.ones((nC,nT),dtype=float)\n",
    "\n",
    "for edge in Interactions_train:\n",
    "    # each edge has \"target-compound-value-active\"\n",
    "    DTI[ Labels_Comp[edge[1]], Labels_Targ[edge[0]] ] = edge[2]\n",
    "\n",
    "values = [x[2] for x in Interactions]\n",
    "print(\"New data: {0} | {1}\".format(np.mean(values), np.std(values)))\n",
    "\n",
    "# load fingerprints and prepare as feature vectors\n",
    "Fingerprints={} # this contains one list per fingerprint - not efficient...\n",
    "with open('Compound_Fingerprints.tab', 'r') as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        # each line is Comp-ID, SMILES, FP\n",
    "        tokens = line.split()\n",
    "        # we keep only those compounds which have FPs\n",
    "        if tokens[2] != 'NOFP':\n",
    "            fp = [int(c) for c in tokens[2] ]\n",
    "            Fingerprints[ tokens[0] ] = fp\n",
    "print(\"%d fingerprints were loaded!\" % len(Fingerprints))\n",
    "\n",
    "# split to train/test data\n",
    "random_seed = 2019\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "print(\"The sizes for train and validation sets are {0} and {1} respectivelly\".format( len(Interactions_train), len(Interactions_valid) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MTL(lamda=0.02, wsl=200, whl=20, lr=0.0001):\n",
    "    inputs = keras.Input(shape=(2048,))\n",
    "    sharedlayer = keras.layers.Dense(wsl, activation='tanh',kernel_regularizer=regularizers.l2(lamda) )(inputs) \n",
    "    myinit = keras.initializers.Constant(4.)\n",
    "    hidden = []\n",
    "    for i in range(len(Targets)):\n",
    "        hl = Dense(units=whl,  activation='tanh', kernel_regularizer=regularizers.l2(lamda) )(sharedlayer)\n",
    "        hidden.append( Dense(1, kernel_initializer=myinit, activity_regularizer=regularizers.l1(0.0001) )(hl) )\n",
    "\n",
    "    MTL=Model(inputs=inputs, outputs=hidden)\n",
    "    MTL.compile(loss=masked_loss_function, optimizer=keras.optimizers.adam(lr=0.0001))\n",
    "    return MTL\n",
    "\n",
    "def masked_loss_function(y_true, y_pred, MissingVal=10):\n",
    "    # This function masks the elements of the vectors with true/predicted values so that the model focuses\n",
    "    # only on the known data. By default, missing values are represented by 10\n",
    "    mask = K.cast(K.not_equal(y_true, MissingVal), K.floatx())\n",
    "    return keras.losses.mean_squared_error(y_true * mask, y_pred * mask)\n",
    "\n",
    "def MTL_Drop( wsl, whl, drop_rate=0.1, lr=0.001):\n",
    "    # a function that creates a NN with dropout incorporated after the first hidden layer\n",
    "    inputs = keras.Input(shape=(2048,))\n",
    "    sharedlayer = keras.layers.Dense(wsl, activation='tanh' )(inputs) \n",
    "    dropout= keras.layers.Dropout(drop_rate)(sharedlayer, training=True)\n",
    "    myinit = keras.initializers.Constant(4.)\n",
    "    hidden = []\n",
    "    for i in range(len(Targets)):\n",
    "        hl = Dense(units=whl,  activation='tanh', kernel_regularizer=regularizers.l2(0.05) )(dropout)\n",
    "        hidden.append( Dense(1, kernel_initializer=myinit, activity_regularizer=regularizers.l1(0.0001) )(hl) )\n",
    "\n",
    "    MTL=Model(inputs=inputs, outputs=hidden)\n",
    "    MTL.compile(loss=masked_loss_function, optimizer=keras.optimizers.adam(lr=lr), )  #metrics=[masked_r2]\n",
    "    return MTL\n",
    "\n",
    "from scipy.stats import sem\n",
    "from scipy.stats import t as tstat\n",
    "\n",
    "def mulpredict(model, x_test, Ntargets, N=10, conf_flag=False):\n",
    "    preds = np.zeros( (N, Ntargets) )\n",
    "    for i in range(N):\n",
    "        preds[i,:] = [ x[0][0] for x in model.predict( x_test ) ]\n",
    "    if conf_flag:\n",
    "        std_err = sem(preds, axis=0)\n",
    "        h = std_err * tstat.ppf((1 + 0.95) / 2, len(preds) - 1)\n",
    "        return np.mean(preds, axis=0), h\n",
    "        # we need the column-wise average of this matrix\n",
    "    else:\n",
    "        return np.mean(preds, axis=0) \n",
    "\n",
    "def Evaluate(Inter_list, Comp_list, Model, Fingerprints, Ntar=110, Niter=10):\n",
    "    Predictions = []\n",
    "    Percomp = {} # contains dicts with lists: (target: [true, pred_NN] )\n",
    "    for test_case in Comp_list:\n",
    "        Percomp[ test_case ] = {}\n",
    "        for tokens in Inter_list:\n",
    "            if tokens[1]==test_case:\n",
    "                Percomp[test_case][ tokens[0] ] = [ tokens[2] ]\n",
    "        if len(Percomp[ test_case ])>0:\n",
    "            # we've got some values for this compound, now produce predictions:\n",
    "            preds = mulpredict(Model, np.array( Fingerprints[test_case]).reshape(1,-1), Ntar, Niter)\n",
    "            \n",
    "            for target in Percomp[test_case]: \n",
    "                Percomp[test_case][target].append( preds[Labels_Targ[target]])\n",
    "                Predictions.append( [target,test_case, Percomp[test_case][target][0], Percomp[test_case][target][1] ])\n",
    "\n",
    "        if len(Predictions) % 1000 == 0:\n",
    "            r2 = r2_score([x[2] for x in Predictions], [x[3] for x in Predictions])\n",
    "            print(f\"\\rMore than \", len(Predictions),\" pairs have been parsed. Mean performance so far =\",r2, end=\" \")\n",
    "    print(\" \")\n",
    "    r2 = r2_score([x[2] for x in Predictions], [x[3] for x in Predictions])\n",
    "    print(\"Performance for MTL-D NN = %f\" % r2)\n",
    "    return Predictions\n",
    "\n",
    "# Initialize sparse matrix - this will be binary\n",
    "DTI = 10*np.ones((nC,nT),dtype=float)\n",
    "\n",
    "for edge in Interactions_train:\n",
    "    # each edge has \"target-compound-value-active\"\n",
    "    DTI[ Labels_Comp[edge[1]], Labels_Targ[edge[0]] ] = edge[2]\n",
    "DTI.shape\n",
    "Compounds_to_test = set([x[1] for x in Interactions_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Task NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "More than  11000  pairs have been parsed. Mean performance so far = 0.6299385706888115  \n",
      "Performance for MTL-D NN = 0.630451\n",
      "Duration per 1000 predictions =  0.025042821405267358\n",
      "R2 for MTL NN = 0.630451\n",
      "MSE for MTL NN = 2.446853\n"
     ]
    }
   ],
   "source": [
    "# best_params = {'wsl': 300, 'whl': 30, 'lamda': 0.1}\n",
    "# Nepochs = 40; BS=64; lr=0.0001\n",
    "# wsl=best_params['wsl']; whl=best_params['whl']; lamda=best_params['lamda']\n",
    "# print(wsl,whl,lamda,Nepochs,BS,lr)\n",
    "\n",
    "# t0=time()\n",
    "# MTLR = MTL(wsl=wsl, whl=whl, lamda=lamda,lr=lr )\n",
    "# MTLR.fit( np.array([Fingerprints[x] for x in Compounds]), [x for x in DTI.T], epochs=Nepochs, batch_size=BS, verbose=0, use_multiprocessing=True )\n",
    "# print(\"Training length with {0} epochs and BS={1} is {2}\".format(Nepochs, BS, time()-t0))\n",
    "\n",
    "MTLR = load_model('MTL-200-30-0.1-model.h5', compile=False) # custom_objects={'masked_loss_function':masked_loss_function})\n",
    "\n",
    "t0=time()\n",
    "Predictions_MTL = Evaluate( Interactions_valid, Compounds_to_test, MTLR, Fingerprints, Niter=1)\n",
    "tref=1000*(time()-t0)/len(Compounds_to_test)/len(Targets)\n",
    "print(\"Duration per 1000 predictions = \",tref )\n",
    "\n",
    "print(\"R2 for MTL NN = %f\" % r2_score([x[2] for x in Predictions_MTL], [x[3] for x in Predictions_MTL]) )\n",
    "print(\"MSE for MTL NN = %f\" %     MSE([x[2] for x in Predictions_MTL], [x[3] for x in Predictions_MTL]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Task with drop-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "More than  11000  pairs have been parsed. Mean performance so far = 0.6220269253385564  \n",
      "Performance for MTL-D NN = 0.622394\n",
      "Duration per 1000 predictions =  0.08990162024360562\n",
      "Relative duration per 1000prd =  3.589915800170034\n",
      "R2 for MTLD NN = 0.622394\n",
      "MSE for MTLD NN = 2.500201\n"
     ]
    }
   ],
   "source": [
    "# best_params = {'wsl': 200, 'whl': 20, 'drop-rate': 0.05}\n",
    "# Nepochs = 40; BS=64; lr=0.0001\n",
    "# wsl=best_params['wsl']; whl=best_params['whl']; drop_rate=best_params['drop-rate']\n",
    "# print(wsl,whl,lamda,Nepochs,BS,lr)\n",
    "\n",
    "# t0=time()\n",
    "# MTLD = MTL_Drop(wsl=wsl, whl=whl, drop_rate=drop_rate,lr=lr )\n",
    "# MTLD.fit( np.array([Fingerprints[x] for x in Compounds]), [x for x in DTI.T], epochs=Nepochs, batch_size=BS, verbose=0, use_multiprocessing=True )\n",
    "# print(\"Training length with {0} epochs and BS={1} is {2}\".format(Nepochs, BS, time()-t0))\n",
    "\n",
    "# MTLD = load_model('MTLD.200.20.005.model.h5', compile=False) # custom_objects={'masked_loss_function':masked_loss_function})\n",
    "MTLD = load_model('MTLD-200-20-0.1-model.h5', compile=False) # custom_objects={'masked_loss_function':masked_loss_function})\n",
    "\n",
    "t0=time()\n",
    "Compounds_to_test = set([x[1] for x in Interactions_valid])\n",
    "Predictions_MTD = Evaluate( Interactions_valid, Compounds_to_test, MTLD, Fingerprints, Niter=5)\n",
    "temp = 1000*(time()-t0)/len(Compounds_to_test)/len(Targets)\n",
    "print(\"Duration per 1000 predictions = \", temp)\n",
    "print(\"Relative duration per 1000prd = \", temp/tref )\n",
    "\n",
    "print(\"R2 for MTLD NN = %f\" % r2_score([x[2] for x in Predictions_MTD], [x[3] for x in Predictions_MTD]) )\n",
    "print(\"MSE for MTLD NN = %f\" %     MSE([x[2] for x in Predictions_MTD], [x[3] for x in Predictions_MTD]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are ready. Duration so far = 2.3746955394744873\n",
      "Duration per 1000 predictions =  3.636970907307026\n",
      "Relative duration per 1000prd =  145.23007805111158\n",
      "R2 for RF NN = 0.647436\n",
      "MSE for RF NN = 2.334393\n"
     ]
    }
   ],
   "source": [
    "RF_all = {}; Y_pred = []; Y_true = []\n",
    "Pertarget_RF = dict() # contains lists with tuples: (true, pred_RF, pred_NN)\n",
    "Predictions_RF = [] # this will contain items as [target, compound, true, prediction]\n",
    "\n",
    "t0=time()\n",
    "for target in Targets:\n",
    "    # load pre-trained models:\n",
    "    with open( TRAINEDMODELS+'RF_'+target+'_'+'pIC50new.sav', 'rb') as f:\n",
    "        RFR = pickle.load( f )\n",
    "    RF_all[target] = RFR        \n",
    "print(\"Models are ready. Duration so far =\",time()-t0)\n",
    "\n",
    "t0=time()\n",
    "for tokens in Interactions_valid:\n",
    "    # 'Target-ID', 'Compound-ID', 'Std-value'  \n",
    "    if tokens[1] in Fingerprints:\n",
    "        Y_true.append( tokens[2] )\n",
    "        x_test = np.array( Fingerprints[tokens[1]] ).reshape(1,-1) # prepare for prediction\n",
    "\n",
    "        model = RF_all[tokens[0]]\n",
    "        Y_pred.append( model.predict( x_test ) )\n",
    "        Predictions_RF.append( [tokens[0], tokens[1], tokens[2], Y_pred[-1][0]] )\n",
    "\n",
    "    if tokens[0] in Pertarget_RF:\n",
    "        Pertarget_RF[tokens[0]].append( (Y_true[-1], Y_pred[-1][0])  )\n",
    "    else:\n",
    "        # first time for this protein\n",
    "        Pertarget_RF[tokens[0]] = [ (Y_true[-1], Y_pred[-1][0]) ]\n",
    "\n",
    "temp = 1000*(time()-t0)/len(Predictions_RF)\n",
    "print(\"Duration per 1000 predictions = \", temp )\n",
    "print(\"Relative duration per 1000prd = \", temp/tref )\n",
    "\n",
    "print(\"R2 for RF NN = %f\" % r2_score([x[2] for x in Predictions_RF], [x[3] for x in Predictions_RF]) )\n",
    "print(\"MSE for RF NN = %f\" %     MSE([x[2] for x in Predictions_RF], [x[3] for x in Predictions_RF]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are ready. Duration so far = 0.08431863784790039\n",
      "Duration per 1000 predictions =  0.11387647604092488\n",
      "Relative duration per 1000prd =  4.547270221596229\n",
      "R2 for LR NN = 0.591053\n",
      "MSE for LR NN = 2.707715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "LR_all = {}; Y_pred = []; Y_true = []\n",
    "Pertarget_LR = dict() # contains lists with tuples: (true, pred_RF, pred_NN)\n",
    "Predictions_LR = [] # this will contain items as [target, compound, true, prediction]\n",
    "\n",
    "t0=time()\n",
    "for target in Targets:\n",
    "    # load pre-trained models:\n",
    "    with open( TRAINEDMODELS+'LR_'+target+'_'+'pIC50new.sav', 'rb') as f:\n",
    "        LR = pickle.load( f )\n",
    "    LR_all[target] = LR        \n",
    "print(\"Models are ready. Duration so far =\",time()-t0)\n",
    "\n",
    "t0=time()\n",
    "for tokens in Interactions_valid:\n",
    "    # 'Target-ID', 'Compound-ID', 'Std-value'  \n",
    "    if tokens[1] in Fingerprints:\n",
    "        Y_true.append( tokens[2] )\n",
    "        x_test = np.array( Fingerprints[tokens[1]] ).reshape(1,-1) # prepare for prediction\n",
    "\n",
    "        model = LR_all[tokens[0]]\n",
    "        Y_pred.append( model.predict( x_test ) )\n",
    "        Predictions_LR.append( [tokens[0], tokens[1], tokens[2], Y_pred[-1][0]] )\n",
    "\n",
    "    if tokens[0] in Pertarget_LR:\n",
    "        Pertarget_LR[tokens[0]].append( (Y_true[-1], Y_pred[-1][0])  )\n",
    "    else:\n",
    "        # first time for this protein\n",
    "        Pertarget_LR[tokens[0]] = [ (Y_true[-1], Y_pred[-1][0]) ]\n",
    "\n",
    "temp = 1000*(time()-t0)/len(Predictions_LR)\n",
    "print(\"Duration per 1000 predictions = \", temp )\n",
    "print(\"Relative duration per 1000prd = \", temp/tref )\n",
    "\n",
    "print(\"R2 for LR NN = %f\" % r2_score([x[2] for x in Predictions_LR], [x[3] for x in Predictions_LR]) )\n",
    "print(\"MSE for LR NN = %f\" %     MSE([x[2] for x in Predictions_LR], [x[3] for x in Predictions_LR]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are ready. Duration so far = 0.20749902725219727\n",
      "Duration per 1000 predictions =  0.7278384585650303\n",
      "Relative duration per 1000prd =  29.06375630710448\n",
      "R2 for skNN = 0.578135\n",
      "MSE for skNN = 2.793246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "MLPR_all = {}; Y_pred = []; Y_true = []\n",
    "Pertarget_MLPR = dict() # contains lists with tuples: (true, pred_RF, pred_NN)\n",
    "Predictions_MLPR = [] # this will contain items as [target, compound, true, prediction]\n",
    "\n",
    "t0=time()\n",
    "for target in Targets:\n",
    "    with open( TRAINEDMODELS+'NN_'+target+'_'+'pIC50new.sav', 'rb') as f:\n",
    "        MLPR = pickle.load( f )\n",
    "    MLPR_all[target] = MLPR        \n",
    "print(\"Models are ready. Duration so far =\",time()-t0)\n",
    "\n",
    "t0=time()\n",
    "for tokens in Interactions_valid:\n",
    "    # 'Target-ID', 'Compound-ID', 'Std-value'  \n",
    "    if tokens[1] in Fingerprints:\n",
    "        Y_true.append( tokens[2] )\n",
    "        x_test = np.array( Fingerprints[tokens[1]] ).reshape(1,-1) # prepare for prediction\n",
    "\n",
    "        model = MLPR_all[tokens[0]]\n",
    "        Y_pred.append( model.predict( x_test ) )\n",
    "        Predictions_MLPR.append( [tokens[0], tokens[1], tokens[2], Y_pred[-1][0]] )\n",
    "\n",
    "    if tokens[0] in Pertarget_MLPR:\n",
    "        Pertarget_MLPR[tokens[0]].append( (Y_true[-1], Y_pred[-1][0])  )\n",
    "    else:\n",
    "        # first time for this protein\n",
    "        Pertarget_MLPR[tokens[0]] = [ (Y_true[-1], Y_pred[-1][0]) ]\n",
    "        \n",
    "temp = 1000*(time()-t0)/len(Predictions_MLPR)\n",
    "print(\"Duration per 1000 predictions = \", temp )\n",
    "print(\"Relative duration per 1000prd = \", temp/tref )\n",
    "\n",
    "print(\"R2 for skNN = %f\" % r2_score([x[2] for x in Predictions_MLPR], [x[3] for x in Predictions_MLPR]) )\n",
    "print(\"MSE for skNN = %f\" %     MSE([x[2] for x in Predictions_MLPR], [x[3] for x in Predictions_MLPR]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models are ready. Duration so far = 85.47444200515747\n",
      "Duration per 1000 predictions =  4.778290158532405\n",
      "Relative duration per 1000prd =  190.8047851799705\n",
      "R2 for myNN = 0.608141\n",
      "MSE for myNN = 2.594569\n"
     ]
    }
   ],
   "source": [
    "myNN_all = {}; Y_pred = []; Y_true = []\n",
    "Pertarget_myNN = dict() # contains lists with tuples: (true, pred_RF, pred_NN)\n",
    "Predictions_myNN = [] # this will contain items as [target, compound, true, prediction]\n",
    "\n",
    "t0=time()\n",
    "for target in Targets:\n",
    "    # load pre-trained models:\n",
    "    with open( TRAINEDMODELS+'Keras/Keras_'+target+'_'+'pIC50model.json', 'r') as jsonf:\n",
    "        json_model = jsonf.read()\n",
    "    myNN = model_from_json(json_model)\n",
    "    myNN.load_weights( TRAINEDMODELS+'Keras/Keras_'+target+'_'+'weights.h5'  )\n",
    "#     myNN.compile(loss='mean_squared_error', optimizer=keras.optimizers.adam(lr=0.001))\n",
    "    myNN_all[target] = myNN        \n",
    "print(\"Models are ready. Duration so far =\",time()-t0)\n",
    "\n",
    "t0=time()\n",
    "for tokens in Interactions_valid:\n",
    "    # 'Target-ID', 'Compound-ID', 'Std-value'  \n",
    "    if tokens[1] in Fingerprints:\n",
    "        Y_true.append( tokens[2] )\n",
    "        x_test = np.array( Fingerprints[tokens[1]] ).reshape(1,-1) # prepare for prediction\n",
    "\n",
    "        model = myNN_all[tokens[0]]\n",
    "        Y_pred.append( model.predict( x_test ) )\n",
    "        Predictions_myNN.append( [tokens[0], tokens[1], tokens[2], Y_pred[-1][0]] )\n",
    "\n",
    "    if tokens[0] in Pertarget_myNN:\n",
    "        Pertarget_myNN[tokens[0]].append( (Y_true[-1], Y_pred[-1][0])  )\n",
    "    else:\n",
    "        # first time for this protein\n",
    "        Pertarget_myNN[tokens[0]] = [ (Y_true[-1], Y_pred[-1][0]) ]\n",
    "\n",
    "temp = 1000*(time()-t0)/len(Predictions_myNN)\n",
    "print(\"Duration per 1000 predictions = \", temp )\n",
    "print(\"Relative duration per 1000prd = \", temp/tref )\n",
    "\n",
    "print(\"R2 for myNN = %f\" % r2_score([x[2] for x in Predictions_myNN], [x[3] for x in Predictions_myNN]) )\n",
    "print(\"MSE for myNN = %f\" %     MSE([x[2] for x in Predictions_myNN], [x[3] for x in Predictions_myNN]) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
