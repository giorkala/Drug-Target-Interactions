{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # this is for making a model like every other in scikit\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# import tensorflow as tf\n",
    "# config = tf.ConfigProto(device_count={\"CPU\": 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 110 targets and 23167 compounds currently loaded with 56392 interactions.\n",
      "New data: -4.604582905766776 | 2.5887050795505413\n",
      "23167 fingerprints were loaded!\n",
      "The sizes for train and validation sets are 45114 and 11278 respectivelly\n"
     ]
    }
   ],
   "source": [
    "Interactions_train = []    \n",
    "with open(\"Interactions_Trainset.tab\",'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        # 'Target-ID', 'Compound-ID', 'pIC50'  \n",
    "        Interactions_train.append( [tokens[0], tokens[1], float(tokens[2]) ])\n",
    "\n",
    "Interactions_valid = []        \n",
    "with open(\"Interactions_Validset.tab\",'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        # 'Target-ID', 'Compound-ID', 'pIC50'  \n",
    "        Interactions_valid.append( [tokens[0], tokens[1], float(tokens[2]) ])\n",
    "\n",
    "Interactions = [x for x in Interactions_train]\n",
    "Interactions.extend(Interactions_valid)\n",
    "# we use a dataframe to quickly sort targets wrt #compounds:\n",
    "DF = pd.DataFrame( Interactions, columns =['Target-ID', 'Compound-ID','Std-value']) \n",
    "temp = DF.groupby(['Target-ID']).agg('count').sort_values(by='Compound-ID') # count the number of molecules\n",
    "Targets = list(temp.index)\n",
    "Compounds = np.unique(DF['Compound-ID'])\n",
    "del temp, DF\n",
    "\n",
    "nT=len(Targets); nC=len(Compounds)\n",
    "\n",
    "print(\"There are {0} targets and {1} compounds currently loaded with {2} interactions.\".format(nT,nC,len(Interactions)))\n",
    "\n",
    "values = [x[2] for x in Interactions]\n",
    "print(\"New data: {0} | {1}\".format(np.mean(values), np.std(values)))\n",
    "\n",
    "# load fingerprints and prepare as feature vectors\n",
    "Fingerprints={} # this contains one list per fingerprint - not efficient...\n",
    "with open('Compound_Fingerprints.tab', 'r') as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        # each line is Comp-ID, SMILES, FP\n",
    "        tokens = line.split()\n",
    "        # we keep only those compounds which have FPs\n",
    "        if tokens[2] != 'NOFP':\n",
    "            fp = [int(c) for c in tokens[2] ]\n",
    "            Fingerprints[ tokens[0] ] = fp\n",
    "print(\"%d fingerprints were loaded!\" % len(Fingerprints))\n",
    "\n",
    "# split to train/test data\n",
    "random_seed = 2019\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "print(\"The sizes for train and validation sets are {0} and {1} respectivelly\".format( len(Interactions_train), len(Interactions_valid) ))\n",
    "\n",
    "Labels_Targ = dict()\n",
    "indx=0\n",
    "for x in Targets:\n",
    "    Labels_Targ[x]=indx\n",
    "    indx+=1\n",
    "    \n",
    "Labels_Comp = dict()\n",
    "indx=0\n",
    "for x in Compounds:\n",
    "    Labels_Comp[x]=indx\n",
    "    indx+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout with MTL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23167, 110)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def masked_loss_function(y_true, y_pred, MissingVal=10):\n",
    "    # This function masks the elements of the vectors with true/predicted values so that the model focuses\n",
    "    # only on the known data. By default, missing values are represented by 10\n",
    "    mask = K.cast(K.not_equal(y_true, MissingVal), K.floatx())\n",
    "    return keras.losses.mean_squared_error(y_true * mask, y_pred * mask)\n",
    "\n",
    "def MTL_Drop( wsl, whl, drop_rate=0.1, lr=0.001):\n",
    "    # a function that creates a NN with dropout incorporated after the first hidden layer\n",
    "    inputs = keras.Input(shape=(2048,))\n",
    "    sharedlayer = keras.layers.Dense(wsl, activation='tanh' )(inputs) \n",
    "    dropout= keras.layers.Dropout(drop_rate)(sharedlayer, training=True)\n",
    "    myinit = keras.initializers.Constant(-4.)\n",
    "    hidden = []\n",
    "    for i in range(len(Targets)):\n",
    "        hl = Dense(units=whl,  activation='tanh', kernel_regularizer=regularizers.l2(0.02) )(dropout)\n",
    "        hidden.append( Dense(1, kernel_initializer=myinit, activity_regularizer=regularizers.l1(0.001) )(hl) )\n",
    "\n",
    "    MTL=Model(inputs=inputs, outputs=hidden)\n",
    "    MTL.compile(loss=masked_loss_function, optimizer=keras.optimizers.adam(lr=lr))\n",
    "    return MTL\n",
    "\n",
    "from scipy.stats import sem\n",
    "from scipy.stats import t as tstat\n",
    "\n",
    "def mulpredict(model, x_test, Ntargets, N=10, conf_flag=False):\n",
    "    preds = np.zeros( (N, Ntargets) )\n",
    "    for i in range(N):\n",
    "        preds[i,:] = [ x[0][0] for x in model.predict( x_test ) ]\n",
    "    if conf_flag:\n",
    "        std_err = sem(preds, axis=0)\n",
    "        h = std_err * tstat.ppf((1 + 0.95) / 2, len(preds) - 1)\n",
    "        return np.mean(preds, axis=0), h\n",
    "        # we need the column-wise average of this matrix\n",
    "    else:\n",
    "        return np.mean(preds, axis=0) \n",
    "\n",
    "def Evaluate(Inter_list, Comp_list, Model, Fingerprints, Ntar=110, Niter=10):\n",
    "    Predictions = []\n",
    "    Percomp = {} # contains dicts with lists: (target: [true, pred_NN] )\n",
    "    for test_case in Comp_list:\n",
    "        Percomp[ test_case ] = {}\n",
    "        for tokens in Inter_list:\n",
    "            if tokens[1]==test_case:\n",
    "                Percomp[test_case][ tokens[0] ] = [ tokens[2] ]\n",
    "        if len(Percomp[ test_case ])>0:\n",
    "            # we've got some values for this compound, now produce predictions:\n",
    "            preds = mulpredict(Model, np.array( Fingerprints[test_case]).reshape(1,-1), Ntar, Niter)\n",
    "            \n",
    "            for target in Percomp[test_case]: \n",
    "                Percomp[test_case][target].append( preds[Labels_Targ[target]])\n",
    "                Predictions.append( [target,test_case, Percomp[test_case][target][0], Percomp[test_case][target][1] ])\n",
    "\n",
    "        if len(Predictions) % 1000 == 0:\n",
    "            r2 = r2_score([x[2] for x in Predictions], [x[3] for x in Predictions])\n",
    "            print(f\"\\rMore than \", len(Predictions),\" pairs have been parsed. Mean performance so far =\",r2, end=\" \")\n",
    "    print(\" \")\n",
    "    r2 = r2_score([x[2] for x in Predictions], [x[3] for x in Predictions])\n",
    "    print(\"Performance for MTL-D NN = %f\" % r2)\n",
    "    return Predictions\n",
    "\n",
    "# Initialize sparse matrix - this will be binary\n",
    "DTI = 10*np.ones((nC,nT),dtype=float)\n",
    "\n",
    "for edge in Interactions_train:\n",
    "    # each edge has \"target-compound-value-active\"\n",
    "    DTI[ Labels_Comp[edge[1]], Labels_Targ[edge[0]] ] = edge[2]\n",
    "DTI.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model and evaluate without self-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /homes/kalantzi/.local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,100,0.05) is 0.3217461966244625\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,100,0.1) is 0.24812095383630348\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,100,0.2) is 0.2399123148197154\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,50,0.05) is 0.2110930841062049\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,50,0.1) is 0.21500517423888846\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,50,0.2) is 0.2202118748500931\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,20,0.05) is 0.182486101722071\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,20,0.1) is 0.19856408089302996\n",
      "Fitting for (shared, hidden, drop_rate)=(2000,20,0.2) is 0.199424985808931\n",
      "Fitting for (shared, hidden, drop_rate)=(300,100,0.05) is 0.5050347980912663\n",
      "Fitting for (shared, hidden, drop_rate)=(300,100,0.1) is 0.35869142568025625\n",
      "Fitting for (shared, hidden, drop_rate)=(300,100,0.2) is 0.22044567604581133\n",
      "Fitting for (shared, hidden, drop_rate)=(300,50,0.05) is 0.1857394355742629\n",
      "Fitting for (shared, hidden, drop_rate)=(300,50,0.1) is 0.9152750068113159\n",
      "Fitting for (shared, hidden, drop_rate)=(300,50,0.2) is 0.2363776463819987\n",
      "Fitting for (shared, hidden, drop_rate)=(300,20,0.05) is 0.16141281306870053\n",
      "Fitting for (shared, hidden, drop_rate)=(300,20,0.1) is 0.16900845693217664\n",
      "Fitting for (shared, hidden, drop_rate)=(300,20,0.2) is 0.1711726877173549\n",
      "Fitting for (shared, hidden, drop_rate)=(200,100,0.05) is 0.22987722411688707\n",
      "Fitting for (shared, hidden, drop_rate)=(200,100,0.1) is 0.3677525890151295\n",
      "Fitting for (shared, hidden, drop_rate)=(200,100,0.2) is 7.52038577341696\n",
      "Fitting for (shared, hidden, drop_rate)=(200,50,0.05) is 0.20999887629067632\n",
      "Fitting for (shared, hidden, drop_rate)=(200,50,0.1) is 0.4259781123184008\n",
      "Fitting for (shared, hidden, drop_rate)=(200,50,0.2) is 0.1962709655117525\n",
      "Fitting for (shared, hidden, drop_rate)=(200,20,0.05) is 0.1598663621317704\n",
      "Fitting for (shared, hidden, drop_rate)=(200,20,0.1) is 0.1665775072370916\n",
      "Fitting for (shared, hidden, drop_rate)=(200,20,0.2) is 0.1692152427515031\n"
     ]
    }
   ],
   "source": [
    "BS=64; Nepochs=50\n",
    "param_grid={'wsl':[2000,300,200],'whl':[100,50,20], 'drop_rate':[0.05,0.1,0.2]}\n",
    "\n",
    "Loss_history = {}\n",
    "best_loss = 10\n",
    "best_params =  {} #{'whl':0, 'lamda':0, 'lamda':}\n",
    "with open(\"Cross-val-MTLdrop.txt\", 'w') as f:\n",
    "    for wsl in param_grid['wsl']:\n",
    "        for whl in param_grid['whl']:\n",
    "            for dr in param_grid['drop_rate']:\n",
    "                MTLR = MTL_Drop(wsl=wsl, whl=whl, drop_rate=dr )\n",
    "                MTLR.fit( np.array([Fingerprints[x] for x in Compounds]), [x for x in DTI.T], epochs=Nepochs, batch_size=BS, validation_split=0.25, verbose=0, use_multiprocessing=True )\n",
    "                loss = np.mean( [MTLR.history.history[x][-1] for x in list(MTLR.history.history)[1:]] )\n",
    "                print(\"Fitting for (shared, hidden, drop_rate)=({0},{1},{2}) is {3}\".format(wsl, whl, dr, loss) )\n",
    "                f.write(\"Fitting for (shared, hidden, drop_rate)=({0},{1},{2}) is {3}\\n\".format(wsl, whl, dr, loss))\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    best_params['wsl'] = wsl\n",
    "                    best_params['whl'] = whl\n",
    "                    best_params['drop_rate'] = dr\n",
    "                Loss_history[(wsl,whl,dr)] = MTLR.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = {'wsl': 200, 'whl': 20, 'drop_rate': 0.05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b798b406222c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mNew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcombo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mLoss_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mNew\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLoss_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcombo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m110\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# get the best parametrisation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "# New = {}\n",
    "# for combo in Loss_history.keys():\n",
    "#     New[combo] = np.mean(Loss_history[combo][-1][:110])\n",
    "#     print(combo,'\\t', np.mean(Loss_history[combo][-1][:110]))\n",
    "# # get the best parametrisation\n",
    "# i=0; max_mean = {}\n",
    "# for param in param_grid:\n",
    "#     best=100\n",
    "#     for val in param_grid[param]:\n",
    "#         estim = []\n",
    "#         for combo in New:\n",
    "#             if combo[i]==val:\n",
    "#                 estim.append( New[combo])\n",
    "#         print(param,'\\t',val,'\\t',np.mean(estim))\n",
    "#         if np.mean(estim)<best:\n",
    "#             best = np.mean(estim)\n",
    "#             max_mean[param] = val\n",
    "#     i+=1\n",
    "# print(max_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.29037669482152\n",
      "21.512064087587273\n"
     ]
    }
   ],
   "source": [
    "combo = (200,20,0.05)\n",
    "keys = list(Loss_history[combo])\n",
    "keys.remove('val_loss')\n",
    "keys.remove('loss')\n",
    "print(Loss_history[combo]['val_loss'][-1])\n",
    "s=0\n",
    "for x in keys[:110]:\n",
    "    s+=Loss_history[combo][x][-1]\n",
    "print(s)\n",
    "# len( [Loss_history[(2000,100,  0.05)][x] for x in range(2,110) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1316437867241376"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean ([ Loss_history[combo][x][-1] for x in keys[:90] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( [ Loss_history[combo][x][-1] for x in keys[:110] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( Loss_history[combo][keys[100]] )\n",
    "plt.plot( Loss_history[combo][keys[211]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aver = np.zeros((1,30))\n",
    "aver_v = np.zeros((1,30))\n",
    "\n",
    "for x in np.arange(4623,4641,2):\n",
    "#     plt.plot(Loss_history[combo]['val_dense_'+str(x)+'_loss'])\n",
    "    plt.plot(Loss_history[combo]['dense_'+str(x)+'_loss'])\n",
    "\n",
    "#     aver+=Loss_history[combo]['dense_'+str(x)+'_loss']\n",
    "#     aver_v+=Loss_history[combo]['val_dense_'+str(x)+'_loss']\n",
    "\n",
    "plt.plot(aver[0], label='Train')\n",
    "plt.plot(aver_v[0], label='Valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss_history[combo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = list(MTLR.history.history.keys())\n",
    "layers.remove(\"val_loss\")\n",
    "layers.remove(\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 20 0.05 50 64\n"
     ]
    }
   ],
   "source": [
    "# max_mean={ 'wsl':200, 'whl':30, 'lamda':0.1}\n",
    "max_mean = best_params\n",
    "# Nepochs = 30; BS=64; lr=0.0001\n",
    "wsl=max_mean['wsl']; whl=max_mean['whl']; dr=max_mean['drop_rate']\n",
    "print(wsl,whl,dr,Nepochs,BS)\n",
    "\n",
    "# we assume that CV for model selection has already been performed!\n",
    "MTLD = MTL_Drop(wsl,whl,dr,0.0001)\n",
    "t0=time()\n",
    "MTLD.fit( np.array([Fingerprints[x] for x in Compounds]), [x for x in DTI.T], epochs=30, batch_size=64, verbose=0, use_multiprocessing=True )\n",
    "print(\"Duration for fitting = \", time()-t0)\n",
    "temp = Evaluate( Interactions_valid, Compounds, MTLD, Fingerprints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta=0.10\n",
    "DTI_new = 10*np.ones((nC,nT),dtype=float)\n",
    "for edge in Interactions_train:\n",
    "    # each edge has \"target-compound-value-active\"\n",
    "    DTI[ Labels_Comp[edge[1]], Labels_Targ[edge[0]] ] = edge[2]\n",
    "DTI.shape\n",
    "# print(\"Initial training and evaluation...\")\n",
    "train_size = len(Interactions_train)\n",
    "count = 1 # just a trigger for the next loop\n",
    "while (train_size<0.5e6) & (count>0):\n",
    "# we need to stop after we have no new predictions or we have enough (10K+)\n",
    "    count=0\n",
    "    for x_new in Compounds:\n",
    "        preds, H = mulpredict(MTLD, np.array( Fingerprints[x_new]).reshape(1,-1), 110, 20, True)\n",
    "        # impute accordingly\n",
    "        for t in range(110):\n",
    "            if (H[t]< theta) & (DTI[Labels_Comp[x_new],t] == 10):\n",
    "                DTI[Labels_Comp[x_new],t] = preds[t] # update the train set\n",
    "                count+=1\n",
    "        if list(Compounds).index(x_new) % 100 == 0:\n",
    "            print(f\"\\rMore than\", list(Compounds).index(x_new) ,\"compounds have been parsed with \",count,\"new values.\", end =\" \")\n",
    "    print(count,\" new values where imputed.\")\n",
    "    train_size += count\n",
    "    \n",
    "    if count >0 :\n",
    "        t0 = time()\n",
    "        MTLD.fit( np.array([Fingerprints[x] for x in Compounds]), [x for x in DTI.T], epochs=30, batch_size=128, verbose=2, use_multiprocessing=True )\n",
    "        print(\"Duration for fitting = \", time()-t0)\n",
    "        temp = Evaluate( Interactions_valid, Compounds, MTLD, Fingerprints)\n",
    "    # repeat if enough points were predicted confidently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture\t: 200   20   0.1\n",
    "# Training prm\t: 0.0001   30   128\n",
    "# Duration =  508.8630199432373\n",
    "# Performance for MTL-D NN = 0.495982\n",
    "\n",
    "# Architecture\t: 2000   50   0.1\n",
    "# Training prm\t: 0.0001   30   128\n",
    "# Training duration =  613.2853746414185\n",
    "# Performance for MTL-D NN = 0.224910\n",
    "\n",
    "# Architecture\t: 300   100   0.05\n",
    "# Training prm\t: 0.0001   30   128\n",
    "# Training duration =  459.5082895755768\n",
    "# Performance for MTL-D NN = 0.413208\n",
    "# MSE= 3.8852603312554295\n",
    "\n",
    "# Architecture\t: 300   100   0.05\n",
    "# Training prm\t: 0.0001   30   128\n",
    "# Training duration =  439.01910161972046\n",
    "# Performance for MTL-D NN = 0.413178\n",
    "# MSE= 3.885457687501225\n",
    "\n",
    "# Architecture\t: 200   20   0.05\n",
    "# Training prm\t: 0.0001   30   128\n",
    "# Training duration =  452.59373211860657\n",
    "# Performance for MTL-D NN = 0.486700\n",
    "# MSE= 3.398656832966635\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
